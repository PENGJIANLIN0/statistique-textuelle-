{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd  \n",
    "from nltk import word_tokenize, ConditionalFreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pengj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pengj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\pengj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pengj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pengj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Ratings                                            Reviews  \\\n",
      "0           0      3.0  It had some laughs, but overall the motivation...   \n",
      "1           1      4.0  WAITING TO EXHALE Waiting, and waiting, and wa...   \n",
      "2           2      4.0  Angela Basset was good as expected, but Whitne...   \n",
      "3           3      5.0  The movie is okay, mediocre might even be the ...   \n",
      "4           4      5.0  I got an opportunity to see Waiting To Exhale ...   \n",
      "\n",
      "          movie_name                                           Resenhas  \\\n",
      "0  Waiting to Exhale  Riu algumas risadas, mas no geral a motivação ...   \n",
      "1  Waiting to Exhale  ESPERANDO PARA EXALAR Esperando, e esperando, ...   \n",
      "2  Waiting to Exhale  Angela Basset foi boa como o esperado, mas Whi...   \n",
      "3  Waiting to Exhale  O filme é bom, medíocre pode até ser a palavra...   \n",
      "4  Waiting to Exhale  Tive a oportunidade de ver Waiting To Exhale p...   \n",
      "\n",
      "                           genres  \\\n",
      "0  ['Comedy', 'Drama', 'Romance']   \n",
      "1  ['Comedy', 'Drama', 'Romance']   \n",
      "2  ['Comedy', 'Drama', 'Romance']   \n",
      "3  ['Comedy', 'Drama', 'Romance']   \n",
      "4  ['Comedy', 'Drama', 'Romance']   \n",
      "\n",
      "                                         Description       emotion  \n",
      "0  Based on Terry McMillan's novel, this film fol...  anticipation  \n",
      "1  Based on Terry McMillan's novel, this film fol...  anticipation  \n",
      "2  Based on Terry McMillan's novel, this film fol...  anticipation  \n",
      "3  Based on Terry McMillan's novel, this film fol...  anticipation  \n",
      "4  Based on Terry McMillan's novel, this film fol...  anticipation  \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('C:/Users/pengj/Desktop/NPL/projet/Movies_Reviews_modified_version1.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews    0\n",
      "genres     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "review_genre_null=df[['Reviews', 'genres']].isnull().sum()\n",
    "print(review_genre_null)\n",
    "df= df[df['genres'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review     genre  note\n",
      "0      [It, had, some, laughs, but, overall, the, mot...    Comedy   3.0\n",
      "0      [It, had, some, laughs, but, overall, the, mot...     Drama   3.0\n",
      "0      [It, had, some, laughs, but, overall, the, mot...   Romance   3.0\n",
      "1      [WAITING, TO, EXHALE, Waiting, and, waiting, a...    Comedy   4.0\n",
      "1      [WAITING, TO, EXHALE, Waiting, and, waiting, a...     Drama   4.0\n",
      "...                                                  ...       ...   ...\n",
      "46171  [As, thrillers, go, there, are, a, few, surpri...     Drama   5.0\n",
      "46171  [As, thrillers, go, there, are, a, few, surpri...  Thriller   5.0\n",
      "46172  [As, thrillers, go, there, are, a, few, surpri...    Action   5.0\n",
      "46172  [As, thrillers, go, there, are, a, few, surpri...     Drama   5.0\n",
      "46172  [As, thrillers, go, there, are, a, few, surpri...  Thriller   5.0\n",
      "\n",
      "[113225 rows x 3 columns]\n",
      "['Comedy', 'Drama', 'Romance', 'Action', 'Crime', 'Thriller', 'Adventure', 'Family', 'Animation', 'Horror', 'Mystery', 'Science Fiction', '', 'Fantasy', 'War', 'Western', 'Music', 'History', 'Foreign', 'Documentary', 'TV Movie']\n"
     ]
    }
   ],
   "source": [
    "df_tokens=pd.DataFrame()\n",
    "df_tokens['review'] = df['Reviews'].apply(lambda x: [word for word in word_tokenize(x) if word not in string.punctuation and word.strip() != ''])\n",
    "df_tokens['genre'] = df['genres'].apply(lambda x: [genre.strip() for genre in x.split(',')]) \n",
    "df_tokens = df_tokens.explode('genre') \n",
    "df_tokens['genre'] = df_tokens['genre'].apply(lambda x: x.strip(\"'[]\"))\n",
    "unique_genres = df_tokens['genre'].unique().tolist()\n",
    "df_tokens['note']=df['Ratings']\n",
    "print(df_tokens)\n",
    "print(unique_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nettoyage des données \n",
    "#surprimer des la ponctuation et des éléments vides\n",
    "def nettoyage_string(tokens):\n",
    "    return [re.sub(r'[@#{}?§/+~.,:…]', ' ', token) for token in tokens]\n",
    "def supprimer_symboles_autour(tokens):\n",
    "    return [re.sub(r'^[\\W_]+|[\\W_]+$', ' ', token) for token in tokens]\n",
    "def supprime_token_vide(tokens):\n",
    "    return [token for token in tokens if token.strip() and not re.fullmatch(r'\\W+', token)]\n",
    "\n",
    "df_tokens['review'] = df_tokens['review'].apply(nettoyage_string)\n",
    "df_tokens['review'] = df_tokens['review'].apply(supprimer_symboles_autour)\n",
    "df_tokens['review'] = df_tokens['review'].apply(supprime_token_vide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [laughs, overall, motivation, characters, inco...\n",
      "0    [laughs, overall, motivation, characters, inco...\n",
      "0    [laughs, overall, motivation, characters, inco...\n",
      "1    [WAITING, EXHALE, Waiting, waiting, waiting, w...\n",
      "1    [WAITING, EXHALE, Waiting, waiting, waiting, w...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#nettoyage des données \n",
    "#supprimer des stopwords \n",
    "stop_words=set(stopwords.words('english'))\n",
    "df_tokens['review']=df_tokens['review'].apply(\n",
    "    lambda tokens: [token for token in tokens if token.lower() not in stop_words])\n",
    "print(df_tokens['review'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comedy', 'Drama', 'Romance', 'Action', 'Crime', 'Thriller', 'Adventure', 'Family', 'Animation', 'Horror', 'Mystery', 'Science Fiction', 'Fantasy', 'War', 'Western', 'Music', 'History', 'Foreign', 'Documentary', 'TV Movie']\n"
     ]
    }
   ],
   "source": [
    "#Distributions conditionnelles des commentaires par rapport aux types de films \n",
    "cfd = ConditionalFreqDist(\n",
    "    (str(genres), word.lower())  \n",
    "    for genres, reviews in zip(df_tokens['genre'], df_tokens['review'])  \n",
    "    for genre in genres  \n",
    "    for word in reviews  \n",
    ")\n",
    "print(cfd.conditions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(laughs, NNS), (overall, JJ), (motivation, NN...\n",
      "0    [(laughs, NNS), (overall, JJ), (motivation, NN...\n",
      "0    [(laughs, NNS), (overall, JJ), (motivation, NN...\n",
      "1    [(WAITING, NNP), (EXHALE, NNP), (Waiting, VBG)...\n",
      "1    [(WAITING, NNP), (EXHALE, NNP), (Waiting, VBG)...\n",
      "Name: pos_tagged, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Etiquetage de parties du discours\n",
    "#Distribution des mots par noms, adjective et verb  etc \n",
    "df_tokens['pos_tagged'] = df_tokens['review'].apply(lambda tokens: nltk.pos_tag(tokens))\n",
    "print(df_tokens['pos_tagged'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [overall, incomprehensible, mad, hypocritical,...\n",
      "0    [overall, incomprehensible, mad, hypocritical,...\n",
      "0    [overall, incomprehensible, mad, hypocritical,...\n",
      "1    [popular, popular, popular, much, easy, potent...\n",
      "1    [popular, popular, popular, much, easy, potent...\n",
      "Name: mot_adj, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Selectioner des mots adjectives pour chaque observation \n",
    "df_tokens['mot_adj'] = df_tokens['pos_tagged'].apply(\n",
    "    lambda tagged_tokens: [word for word, pos in tagged_tokens if pos in ['JJ', 'JJR', 'JJS']]\n",
    ")\n",
    "print(df_tokens['mot_adj'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comedy', 'Drama', 'Romance', 'Action', 'Crime', 'Thriller', 'Adventure', 'Family', 'Animation', 'Horror', 'Mystery', 'Science Fiction', '', 'Fantasy', 'War', 'Western', 'Music', 'History', 'Foreign', 'Documentary', 'TV Movie']\n"
     ]
    }
   ],
   "source": [
    "#Distributions conditionnelles des mots adjectives  par rapport aux types de films \n",
    "cfd_adj = ConditionalFreqDist(\n",
    "    (genre, word.lower())  \n",
    "    for genre, adj_words in zip(df_tokens['genre'], df_tokens['mot_adj'])  \n",
    "    for word in adj_words  \n",
    ")\n",
    "print(cfd_adj.conditions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 overall  incomprehensible    mad  hypocritical  messed  \\\n",
      "Comedy             824.0              14.0   84.0          17.0     5.0   \n",
      "Drama             1377.0              25.0  106.0          11.0     3.0   \n",
      "Romance           1131.0              16.0   94.0          14.0     3.0   \n",
      "Action             313.0               8.0   19.0           1.0     1.0   \n",
      "Crime              229.0              10.0    9.0           0.0     0.0   \n",
      "Thriller           470.0              30.0   39.0           2.0     2.0   \n",
      "Adventure          223.0              10.0   16.0           0.0     0.0   \n",
      "Family             266.0               1.0   38.0           0.0     0.0   \n",
      "Animation           99.0               0.0   18.0           0.0     0.0   \n",
      "Horror             280.0              12.0   75.0           2.0     1.0   \n",
      "Mystery            190.0              14.0    8.0           2.0     1.0   \n",
      "Science Fiction    194.0              10.0   17.0           1.0     1.0   \n",
      "                    46.0               2.0    4.0           0.0     0.0   \n",
      "Fantasy            365.0              10.0   52.0           2.0     1.0   \n",
      "War                 37.0               0.0    3.0           0.0     0.0   \n",
      "Western             28.0               1.0    1.0           0.0     0.0   \n",
      "Music              118.0               0.0    5.0           0.0     1.0   \n",
      "History             34.0               2.0    0.0           0.0     0.0   \n",
      "Foreign             86.0               0.0   11.0           1.0     0.0   \n",
      "Documentary         55.0               0.0    4.0           0.0     1.0   \n",
      "TV Movie            15.0               0.0    1.0           0.0     0.0   \n",
      "\n",
      "                 stupid  empathy  popular    much   easy  ...  hard-right  \\\n",
      "Comedy            801.0     30.0    434.0  4698.0  568.0  ...         0.0   \n",
      "Drama             870.0     86.0    494.0  7167.0  780.0  ...         0.0   \n",
      "Romance           722.0     70.0    464.0  5926.0  633.0  ...         0.0   \n",
      "Action            390.0      5.0    122.0  1996.0  333.0  ...         0.0   \n",
      "Crime             181.0      7.0     92.0  1207.0  131.0  ...         0.0   \n",
      "Thriller          400.0     14.0    138.0  2508.0  268.0  ...         0.0   \n",
      "Adventure         241.0      8.0    109.0  1631.0  285.0  ...         0.0   \n",
      "Family            193.0      5.0    114.0  1503.0   77.0  ...         0.0   \n",
      "Animation          70.0      1.0     38.0   544.0   34.0  ...         0.0   \n",
      "Horror            261.0      6.0     84.0  1492.0   90.0  ...         0.0   \n",
      "Mystery           157.0      2.0     51.0   909.0   74.0  ...         0.0   \n",
      "Science Fiction   169.0      6.0     51.0   955.0  100.0  ...         0.0   \n",
      "                   29.0      0.0     23.0   286.0   23.0  ...         0.0   \n",
      "Fantasy           241.0      9.0    129.0  1833.0   97.0  ...         0.0   \n",
      "War                35.0      2.0      5.0   173.0   21.0  ...         0.0   \n",
      "Western            28.0      1.0      8.0   132.0   14.0  ...         1.0   \n",
      "Music             111.0      1.0     60.0   651.0   60.0  ...         0.0   \n",
      "History            65.0      1.0     57.0   291.0   28.0  ...         0.0   \n",
      "Foreign            44.0      0.0     19.0   283.0   38.0  ...         0.0   \n",
      "Documentary        42.0      0.0      7.0   254.0   24.0  ...         1.0   \n",
      "TV Movie           11.0      1.0     14.0   153.0   10.0  ...         0.0   \n",
      "\n",
      "                 wing-nut  hood-winked  transferred  scenarios  teens young  \\\n",
      "Comedy                0.0          0.0          0.0        0.0          0.0   \n",
      "Drama                 0.0          0.0          0.0        0.0          0.0   \n",
      "Romance               0.0          0.0          0.0        0.0          0.0   \n",
      "Action                0.0          0.0          0.0        0.0          0.0   \n",
      "Crime                 0.0          0.0          0.0        0.0          0.0   \n",
      "Thriller              0.0          0.0          0.0        0.0          0.0   \n",
      "Adventure             0.0          0.0          0.0        0.0          0.0   \n",
      "Family                0.0          0.0          0.0        0.0          0.0   \n",
      "Animation             0.0          0.0          0.0        0.0          0.0   \n",
      "Horror                0.0          0.0          0.0        0.0          0.0   \n",
      "Mystery               0.0          0.0          0.0        0.0          0.0   \n",
      "Science Fiction       0.0          0.0          0.0        0.0          0.0   \n",
      "                      0.0          0.0          0.0        0.0          0.0   \n",
      "Fantasy               0.0          0.0          0.0        0.0          0.0   \n",
      "War                   0.0          0.0          0.0        0.0          0.0   \n",
      "Western               1.0          1.0          1.0        3.0          0.0   \n",
      "Music                 0.0          0.0          0.0        0.0          3.0   \n",
      "History               0.0          0.0          0.0        0.0          0.0   \n",
      "Foreign               0.0          0.0          0.0        0.0          0.0   \n",
      "Documentary           1.0          1.0          1.0        0.0          3.0   \n",
      "TV Movie              0.0          0.0          0.0        0.0          0.0   \n",
      "\n",
      "                 pop-artist  audible the  production but  weird one  \n",
      "Comedy                  0.0          0.0             0.0        0.0  \n",
      "Drama                   0.0          0.0             0.0        0.0  \n",
      "Romance                 0.0          0.0             0.0        0.0  \n",
      "Action                  0.0          0.0             0.0        0.0  \n",
      "Crime                   0.0          0.0             0.0        0.0  \n",
      "Thriller                0.0          0.0             0.0        0.0  \n",
      "Adventure               0.0          0.0             0.0        0.0  \n",
      "Family                  0.0          0.0             0.0        0.0  \n",
      "Animation               0.0          0.0             0.0        0.0  \n",
      "Horror                  0.0          0.0             0.0        0.0  \n",
      "Mystery                 0.0          0.0             0.0        0.0  \n",
      "Science Fiction         0.0          0.0             0.0        0.0  \n",
      "                        0.0          0.0             0.0        0.0  \n",
      "Fantasy                 0.0          0.0             0.0        0.0  \n",
      "War                     0.0          0.0             0.0        0.0  \n",
      "Western                 0.0          0.0             0.0        0.0  \n",
      "Music                   0.0          0.0             0.0        0.0  \n",
      "History                 0.0          0.0             0.0        0.0  \n",
      "Foreign                 0.0          0.0             0.0        0.0  \n",
      "Documentary             1.0          1.0             1.0        1.0  \n",
      "TV Movie                0.0          0.0             0.0        0.0  \n",
      "\n",
      "[21 rows x 33302 columns]\n",
      "overall             369.656140\n",
      "incomprehensible      8.644569\n",
      "mad                  33.625741\n",
      "hypocritical          4.955997\n",
      "messed                1.321975\n",
      "                       ...    \n",
      "teens young           0.902378\n",
      "pop-artist            0.218218\n",
      "audible the           0.218218\n",
      "production but        0.218218\n",
      "weird one             0.218218\n",
      "Length: 33302, dtype: float64\n",
      "Index(['character she', 'good there', 'lite-comedy', 'belittle',\n",
      "       'fair performance-wise', 'sincerity', 'spots for', 'schlock-fest i',\n",
      "       'fra'nk', 'teener-bopper',\n",
      "       ...\n",
      "       'wing', 'bashed', 'hard-right', 'wing-nut', 'hood-winked',\n",
      "       'transferred', 'pop-artist', 'audible the', 'production but',\n",
      "       'weird one'],\n",
      "      dtype='object', length=15310)\n"
     ]
    }
   ],
   "source": [
    "#Construction une dataframe pour des mots adjectives par rapport aux types de films\n",
    "df_adj_frequant = pd.DataFrame(\n",
    "    {genre: dict(cfd_adj[genre]) for genre in cfd_adj.conditions()}  \n",
    ").fillna(0)\n",
    "df_adj_frequant = df_adj_frequant.T\n",
    "print(df_adj_frequant)\n",
    "\n",
    "#Selectionner des mots adjectives très cumulatives pour les supprimer par raport aux leur std \n",
    "std_valeur=df_adj_frequant.std()\n",
    "print(std_valeur)\n",
    "mot_frequante=std_valeur[std_valeur<0.5].index\n",
    "print(mot_frequante)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition d'une fonction d'obtenir des dataframes des mots adjectives pour chaque type de film \n",
    "def genre_data(df, genre):\n",
    "    genre_df = df[df['genre'] == genre][['mot_adj', 'note']]\n",
    "    genre_df.reset_index(drop=True, inplace=True)\n",
    "    list_adj = set([adj for sublist in genre_df['mot_adj'] for adj in sublist])\n",
    "    return genre_df, list_adj\n",
    "\n",
    "comedy_df, unique_adj_comedy = genre_data(df_tokens, 'Comedy')\n",
    "horror_df, unique_adj_horror=genre_data(df_tokens,'Horror')\n",
    "thriller_df, unique_adj_thriller=genre_data(df_tokens,'Thriller')\n",
    "animation_df, unique_adj_animation=genre_data(df_tokens,'Animation')\n",
    "romance_df, unique_adj_romance=genre_data(df_tokens,'Romance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire une bocule pour prendre toutes les données pour chaque type de film \n",
    "#Et 70% des échantilonnages pour chaque type de film sont distribuées être le training échantillonnage; \n",
    "# le reste 30% des échantillonages our chaque type de film sont distribuées être le test échantillonnage\n",
    "toute_genre_data= {}\n",
    "for genre in cfd_adj.conditions():\n",
    "    genre_df, unique_adj = genre_data(df_tokens, genre)\n",
    "    train_data, test_data = train_test_split(genre_df, test_size=0.3, random_state=42)\n",
    "\n",
    "    toute_genre_data[genre] = {\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'unique_adjectives': unique_adj\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire une boucle pour obtenir toutes les fréquentes des mots adjective pour chaque type de film d'échantillonage de training\n",
    "toute_genre_adjective_freq = {}\n",
    "for genre in cfd_adj.conditions():\n",
    "    train_data = toute_genre_data[genre]['train_data']\n",
    "    all_adjectives = [adj for sublist in train_data['mot_adj'] for adj in sublist]\n",
    "    adj_select = [adj for adj in all_adjectives if adj not in [' s', ' m']]\n",
    "    adj_frequency = Counter(adj_select)\n",
    "    cent_adjectives_freq = adj_frequency.most_common(100)\n",
    "    cent_adjectives= [adj[0] for adj in cent_adjectives_freq]\n",
    "    cinqcent_adjectives_freq = adj_frequency.most_common(500)\n",
    "    cinqcent_adjectives= [adj[0] for adj in cinqcent_adjectives_freq]\n",
    "\n",
    "    toute_genre_adjective_freq[genre] = {\n",
    "        'frequency': adj_frequency,\n",
    "        'adjectives': adj_select,\n",
    "        'cent_adj': cent_adjectives,\n",
    "        'cinqcent_adj':cinqcent_adjectives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition d'une fonction de sentiment des mots adjectives \n",
    "def sentiment_point(word, pos):\n",
    "    if pos == 'a':  \n",
    "        synsets = list(swn.senti_synsets(word, 'a'))\n",
    "    else:\n",
    "        return None  \n",
    "    if synsets:\n",
    "        sentiment = synsets[0]  \n",
    "        positive = sentiment.pos_score()  \n",
    "        negative = sentiment.neg_score()  \n",
    "        objective = sentiment.obj_score() \n",
    "        total_score = positive - negative  \n",
    "        return total_score\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculer des nombres des mots, des nombres de mot négatives et postives  par chaque comentaires \n",
    "def calculate_sentiments(row):\n",
    "    words = row['mot_adj']\n",
    "    word_count = len(words)\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        score = sentiment_point(word, 'a')\n",
    "        if score > 0:\n",
    "            positive_count += 1\n",
    "        elif score < 0:\n",
    "            negative_count += 1\n",
    "    \n",
    "    positive_ratio = positive_count / word_count if word_count > 0 else 0\n",
    "    negative_ratio = negative_count / word_count if word_count > 0 else 0\n",
    "    \n",
    "    return pd.Series({\n",
    "        'nombre_mot': word_count,\n",
    "        'nombre_positive': positive_count,\n",
    "        'nombre_negative': negative_count,\n",
    "        'positive_ratio': positive_ratio,\n",
    "        'negative_ratio': negative_ratio\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation des points de sentiments totales pour chaque observation à l'échantilonnage \n",
    "def calculate_total_sentiment(adjectives, unique_adjectives):\n",
    "    total_score = 0  \n",
    "    for adj in adjectives:\n",
    "        if adj in unique_adjectives: \n",
    "            total_score += sentiment_point(adj, 'a')  \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer des notes à la variable binomale: 0 et 1 par rapport sa valeur de median\n",
    "def binarize_by_median(df, note, median_value):\n",
    "    binary_column = df[note].apply(lambda x: 1 if x >= median_value else 0)\n",
    "    return binary_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition d'une fonction pour obtenir des points de sentiments pour chaque observtion\n",
    "def create_adj_sentiment_matrix(train_df, cent_adj):\n",
    "    sentiment_matrix = pd.DataFrame(0.0, index=train_df.index, columns=cent_adj)  \n",
    "\n",
    "    for index, row in train_df.iterrows():\n",
    "        adjectives = row['mot_adj']\n",
    "        for adj in adjectives:\n",
    "            if adj in cent_adj:\n",
    "                sentiment_score = sentiment_point(adj, 'a')  \n",
    "                sentiment_matrix.at[index, adj] = sentiment_score  \n",
    "    return sentiment_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 mot_adj  note\n",
      "10806  [better, ensemble, short, full-length, god-awf...   5.0\n",
      "10784  [different, turned, big, stupid, much, little,...   4.0\n",
      "11323  [poor, planet, little, romantic-comedy, enough...   5.0\n",
      "10715  [non-stop, off-color, early, first, object, hi...   8.0\n",
      "11452  [etc ,  stellar, watchable, horrendous, script...   3.0\n",
      "...                                                  ...   ...\n",
      "13418  [care, follow, next, hairy, top, heavy, differ...   7.0\n",
      "5390   [stupid, detective, good, terrific, simple, gr...   6.0\n",
      "860    [last, dandy, little, al, inked, late, next, b...  10.0\n",
      "15795  [sure, inordinate, much, good, hideous, light,...   2.0\n",
      "7270   [user, good, bad, thought, great, funny, good,...   7.0\n",
      "\n",
      "[11136 rows x 2 columns]\n",
      "['good', 'great', 'funny', 'much', 'bad', 'many', 'little', 'best', 'romantic', 'new', 'real', 'original', 'first', 'old', 'big', 'whole', 'different', 'young', 'nice', 'least', 'last', 'better', 'sure', 'high', 'hilarious', 'main', 'hard', 'right', 'beautiful', 'wrong', 'true', 'perfect', 'predictable', 'second', 'enough', 'classic', 'happy', 'special', 'entire', 'long', 'enjoyable', 'small', 'excellent', 'worth', 'serious', 'full', 'able', 'next', 'top', 'worst', 'screen', 'wonderful', 'typical', 'stupid', 'comic', 'interesting', 'give', 'short', 'strong', 'decent', 'poor', 'single', 'low', 'fine', 'simple', 'usual', 'worse', 'likable', 'overall', 'comedic', 'several', 'emotional', 'terrible', 'American', 'musical', 'believable', 'huge', 'ridiculous', 'awful', 'complete', 'similar', 'easy', 'fun', 'fantastic', 'obvious', 'teen', 'modern', 'live', 'previous', 'less', 'sweet', 'important', 'disappointed', 'average', 'human', 'early', 'female', 'due', 'see', 'major']\n",
      "['good', 'great', 'funny', 'much', 'bad', 'many', 'little', 'best', 'romantic', 'new', 'real', 'original', 'first', 'old', 'big', 'whole', 'different', 'young', 'nice', 'least', 'last', 'better', 'sure', 'high', 'hilarious', 'main', 'hard', 'right', 'beautiful', 'wrong', 'true', 'perfect', 'predictable', 'second', 'enough', 'classic', 'happy', 'special', 'entire', 'long', 'enjoyable', 'small', 'excellent', 'worth', 'serious', 'full', 'able', 'next', 'top', 'worst', 'screen', 'wonderful', 'typical', 'stupid', 'comic', 'interesting', 'give', 'short', 'strong', 'decent', 'poor', 'single', 'low', 'fine', 'simple', 'usual', 'worse', 'likable', 'overall', 'comedic', 'several', 'emotional', 'terrible', 'American', 'musical', 'believable', 'huge', 'ridiculous', 'awful', 'complete', 'similar', 'easy', 'fun', 'fantastic', 'obvious', 'teen', 'modern', 'live', 'previous', 'less', 'sweet', 'important', 'disappointed', 'average', 'human', 'early', 'female', 'due', 'see', 'major', 'brilliant', 'weak', 'successful', 'girl', 'realistic', 'black', 'lead', 'favorite', 'laugh', 'negative', 'horrible', 'fresh', 'final', 'entertaining', 'biggest', 'cool', 'certain', ' re', 'slow', 'hot', 'popular', 'rich', 'potential', 'flat', 'light', 'gorgeous', 'couple', 'social', 'familiar', 'white', 'sexual', 'recent', 'deep', 'possible', 'watch', 'loud', 'awesome', 'dramatic', 'mean', 'positive', 'personal', 'strange', 'actual', 'funniest', 'Indian', 'wish', 'particular', 'amazing', 'want', 'know', 'cute', 'memorable', 'middle', 'younger', 'unique', 'smart', 'total', 'clear', 'normal', 'cast', 'solid', 'script', 'general', 'large', 'former', 'impressive', 'difficult', 'past', 'famous', 'like', 'humorous', 'sad', 'common', 'crazy', 'open', 'honest', 'present', 'future', 'older', 'interested', 'watchable', 'weird', 'type', 'attractive', 'fair', 'third', 'liked', 'British', 'close', 'end', 'write', 'delightful', 'cheap', 'scary', 'dumb', 'male', 'painful', 'character', 'shot', 'various', 'natural', 'dark', 'incredible', ' ll', 'narrative', 'boring', 'enjoy', 'effective', 'dead', 'married', 'free', 'rare', 'intelligent', 'pathetic', 'late', 'enjoyed', 'unfunny', 'surprised', 'local', 'dull', 'physical', 'outstanding', 'understand', 'pleasant', 'half', 'plenty', 'French', 'smile', 'unnecessary', 'witty', 'constant', 'over-the-top', 'wild', 'basic', 'subject', 'necessary', 'unbelievable', 'unexpected', 'greatest', 'love', 'odd', 'flick', 'adorable', 'impossible', 'cinematic', 'saw', 'minor', 'Happy', 'genuine', ' ve', 'heavy', 'visual', 'pretty', 'worthy', 'gross', 'numerous', 'surprising', 'OK', 'likely', 'lovable', 'Good', 'clever', 'guy', 'bigger', 'latest', 'creative', 'silly', 'read', 'shallow', 'willing', 'touch', 'slapstick', 'straight', 'quirky', 'subtle', 'forgettable', 'innocent', 'secret', 'capable', 'public', 'thin', 'funnier', 'tale', 'lame', 'higher', 'mixed', 'moral', 'novel', 'fabulous', ' d', 'happen', 'direct', 'evil', 'fantasy', 'come', 'else', 'tough', 'ready', 'thought', 'comedy', 'terrific', 'actress', 'sci-fi', 'loose', 'impressed', 'magic', 'Many', 'Overall', 'fairy', 'sexy', 'material', 'Last', 'unrealistic', 'fat', 'ensemble', 'standard', 'sorry', 'front', 'superficial', 'disappointing', 'ten', 'start', 'dry', 'offensive', 'quiet', 'opposite', 'obnoxious', 'political', 'clean', 'central', 'magical', 'safe', 'empty', 'act', 'storyline', 'unoriginal', 'chick', 'sympathetic', 'fit', 'current', 'miss', 'professional', 'drunk', 'uncomfortable', 'complex', 'sensitive', 'spectacular', 'curious', 'guess', 'comedian', 'brief', 'extra', 'follow', 'initial', 'old-fashioned', 'cheesy', 'light-hearted', 'ordinary', 'Italian', 'intense', 'awkward', 'comfortable', 'stereotypical', 'hysterical', 'childish', 'quick', 'rom-com', 'hear', 'aware', 'song', 'regular', 'grand', 'cold', 'technical', 'warm', 'alive', 'french', 'guilty', 'viewer', 'plot', 'talent', 'generic', 'humour', 'wealthy', 'stick', 'cinema', 'scene', 'bit', 'unusual', 'psychological', 'charismatic', 'comical', 'Sure', 'watched', 'bright', 'wacky', 'mysterious', 'sudden', 'left', 'nasty', 'rest', 'table', 'bizarre', 'logic', 'specific', 'suppose', 'appropriate', 'genre', 'lucky', 'super', 'mediocre', 'tremendous', 'commercial', 'double', 'notable', 'religious', 'dynamic', 'finish', 'appreciate', 'sheep', 'mainstream', 'wide', 'independent', 'bunch', 'superior', 'logical', 'keep', 'traditional', 'naked', 'powerful', 'on-screen', 'pregnant', 'boy', 'hate', 'become', 'desperate', 'ultimate', 'critic', 'English', 'corporate', 'stuff', 'key', 'finale', 'animal', 'sick', 'tired', 'make', 'expensive', 'underrated', 'favourite', 'fake', 'cynical', 'minute', 'purpose', 'soft', 'recommend', 'facial', 'lose', 'feel-good', 'silent', 'pick', 'universe', 'theatrical', 'absurd', 'title', 'colorful', 'artistic', 'unknown', 'agree', 'superb', 'unable', 'angry', 'acceptable', 'pointless', 'gay', 'massive', 'rental', 'outrageous', 'okay', 'mental', 'stop', 'pull', 'avoid', 'sarcastic', 'wanted', 'lazy', 'aspect', 'u', 'individual', 'idiotic', 'lot', 'suspenseful', 'afraid', 'porn', 'remarkable', 'laughable', 'prepared', 'theater', 'fellow', 'faithful']\n"
     ]
    }
   ],
   "source": [
    "comedy_train_df = toute_genre_data['Comedy']['train_data']\n",
    "comedy_unique_adjectives = toute_genre_data['Comedy']['unique_adjectives']\n",
    "comedy_cent_adj = toute_genre_adjective_freq.get('Comedy', {}).get('cent_adj', [])\n",
    "comedy_cinqcent_adj = toute_genre_adjective_freq.get('Comedy', {}).get('cinqcent_adj', [])\n",
    "print(comedy_train_df)\n",
    "print(comedy_cent_adj)\n",
    "print(comedy_cinqcent_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                mot_adj  note\n",
      "1614  [aware, avid, higher, first, cheap, predictabl...   9.0\n",
      "336   [ The, directorial, starred, first, admirable,...   8.0\n",
      "2505  [usual, correct, brush, chitty, real, sick, Ca...   2.0\n",
      "2664  [absolute, follow, self-imposed, many, book-ba...   2.0\n",
      "3795  [allow, low, little, make, good, small, simila...   1.0\n",
      "...                                                 ...   ...\n",
      "4426  [difficult, bad, better, talentless, watch, gr...   1.0\n",
      "466   [deep, old, French, serious, serious, dark, su...   6.0\n",
      "3092  [trailer, shot, clusterfluck, fair, fair, horr...   1.0\n",
      "3772  [bad, good, independent, tight, awful Let, man...   1.0\n",
      "860        [diabolical, enjoyed, good, recommend, good]  10.0\n",
      "\n",
      "[3420 rows x 2 columns]\n",
      "['good', 'bad', 'great', 'much', 'many', 'little', 'best', 'original', 'old', 'new', 'first', 'real', 'least', 'scary', 'young', 'whole', 'better', 'different', 'big', 'last', 'funny', 'main', 'low', 'nice', 'beautiful', 'classic', 'worst', 'sure', 'special', 'right', 'enough', 'wrong', 'terrible', 'decent', 'poor', 'small', 'true', 'next', 'high', 'entire', 'second', 'interesting', 'excellent', 'dark', 'worth', 'full', 'serious', 'short', 'predictable', 'able', 'several', 'dead', 'human', 'give', 'obvious', 'long', 'stupid', 'awful', 'hard', 'top', 'slow', 'strange', 'overall', 'final', 'single', 'possible', 'usual', 'strong', 'enjoyable', 'ridiculous', 'horrible', 'romantic', 'worse', 'previous', 'huge', 'believable', 'American', 'simple', 'early', 'evil', 'modern', 'fine', 'perfect', 'fantastic', 'similar', 'horror', 'supernatural', 'clear', 'due', 'potential', ' re', 'cheap', 'screen', 'weird', 'typical', 'memorable', 'weak', 'average', 'happy', 'complete']\n",
      "['good', 'bad', 'great', 'much', 'many', 'little', 'best', 'original', 'old', 'new', 'first', 'real', 'least', 'scary', 'young', 'whole', 'better', 'different', 'big', 'last', 'funny', 'main', 'low', 'nice', 'beautiful', 'classic', 'worst', 'sure', 'special', 'right', 'enough', 'wrong', 'terrible', 'decent', 'poor', 'small', 'true', 'next', 'high', 'entire', 'second', 'interesting', 'excellent', 'dark', 'worth', 'full', 'serious', 'short', 'predictable', 'able', 'several', 'dead', 'human', 'give', 'obvious', 'long', 'stupid', 'awful', 'hard', 'top', 'slow', 'strange', 'overall', 'final', 'single', 'possible', 'usual', 'strong', 'enjoyable', 'ridiculous', 'horrible', 'romantic', 'worse', 'previous', 'huge', 'believable', 'American', 'simple', 'early', 'evil', 'modern', 'fine', 'perfect', 'fantastic', 'similar', 'horror', 'supernatural', 'clear', 'due', 'potential', ' re', 'cheap', 'screen', 'weird', 'typical', 'memorable', 'weak', 'average', 'happy', 'complete', 'wonderful', 'less', 'brilliant', 'shot', 'black', 'mysterious', 'personal', 'comic', 'solid', 'effective', 'biggest', 'emotional', 'visual', 'major', 'unique', 'disappointed', 'middle', 'certain', 'dull', 'cool', 'particular', 'know', 'psychological', 'recent', 'negative', 'lead', 'deep', 'novel', 'realistic', 'general', 'actual', 'female', 'difficult', 'famous', 'former', 'creepy', 'flat', 'fun', 'script', 'suspenseful', 'wish', 'intense', 'third', 'open', 'important', 'crazy', 'interested', 'entertaining', 'dramatic', 'musical', 'half', 'familiar', ' ve', 'fresh', 'subject', 'see', 'couple', 'hilarious', 'common', 'late', 'free', 'gorgeous', 'light', 'powerful', 'flick', 'total', 'sad', 'past', ' ll', 'favorite', 'sci-fi', 'read', 'live', 'likable', 'Julian', 'future', 'suspense', 'write', 'successful', 'large', 'end', 'impressive', 'amazing', 'understand', 'hot', 'local', 'surprised', 'thought', 'teen', 'normal', 'mean', 'watchable', 'unnecessary', 'alive', 'positive', 'awesome', 'bizarre', 'various', 'heavy', 'boring', 'natural', 'want', 'tale', 'happen', 'viewer', 'fair', 'genre', 'girl', 'mental', 'surprising', 'unexpected', 'like', 'twist', 'higher', 'close', 'watch', 'easy', 'white', 'generic', 'liked', 'low-budget', 'pretty', 'make', 'honest', 'narrative', 'watched', 'fairy', 'type', 'nasty', 'over-the-top', 'popular', 'painful', 'willing', 'Last', 'intelligent', 'basic', 'present', 'Many', 'sweet', 'bigger', 'impossible', 'incredible', 'violent', 'angry', 'standard', 'come', 'smart', 'start', 'secret', 'OK', 'alien', 'older', 'wooden', 'central', 'fake', 'tough', 'mad', 'forgettable', 'complex', 'graphic', 'unbelievable', 'cinematic', 'red', 'cathedral', 'odd', 'physical', 'constant', 'tense', 'detective', 'else', 'sexual', 'atmospheric', 'storyline', 'creative', 'quiet', 'moral', 'survive', 'minor', 'latest', 'outstanding', 'empty', 'fit', 'genuine', 'follow', 'numerous', 'attractive', 'make-up', 'guess', 'capable', 'ready', 'animated', 'flesh', 'British', 'suitable', 'cast', 'plenty', 'unknown', 'tragic', 'pathetic', 'sleep', 'subtle', 'enjoyed', 'religious', 'enjoy', 'loud', 'silly', 'younger', 'faithful', 'innocent', 'responsible', 'magic', 'laughable', 'likely', 'straight', 'glad', 'Japanese', 'disappointing', 'wild', 'sound', 'Overall', 'worthy', 'male', 'gory', 'scared', 'independent', 'artificial', 'spectacular', 'adult', 'rich', 'greatest', 'love', 'aspect', 'recommend', 'bit', ' The', 'rare', 'humorous', 'necessary', 'aware', 'outside', 'gross', 'clever', 'gruesome', 'character', 'forest', 'unusual', 'fantasy', 'impressed', 'dumb', 'extra', 'front', 'title', 'mixed', 'underrated', 'magnificent', 'stick', 'married', 'sick', 'hideous', 'sympathetic', 'touch', 'artistic', 'Italian', 'claustrophobic', 'gratuitous', 'serial', 'inherent', 'actress', 'brief', 'loose', 'psychotic', 'directorial', 'specific', 'saw', ' d', 'regular', 'finale', 'naked', 'proper', 'earlier', 'limited', 'sudden', 'non-existent', 'tight', 'bright', 'find', 'curious', 'crappy', 'passable', 'mediocre', 'extreme', 'become', 'cold', 'ten', 'stuff', 'wasted', 'available', 'latter', 'magical', 'act', 'stop', 'multiple', 'giant', 'die', 'sense', 'lavish', 'traditional', 'porn', 'lame', 'drunk', 'Egyptian', 'sheep', 'quick', 'eternal', 'superior', 'massive', 'documentary', 'unable', 'Asian', 'Australian', 'appropriate', 'handsome', 'repetitive', 'tried', 'rival', 'apparent', 'laugh', 'favourite', 'dangerous', 'cute', 'bunch', 'horrific', 'brutal', 'erotic', 'remarkable', 'terrific', 'left', 'acting', 'oh', 'pointless', 'amateurish', 'notice', 'suppose', 'acceptable', 'tiny', 'teenage', 'unfortunate', 'technical', 'atmosphere', 'contemporary', 'footage', 'hear', 'nonsensical', 'desperate', 'adequate', 'opposite', 'direct', 'intentional', 'scare', 'fear', 'corporate', 'avoid', 'lousy', 'random', 'darkness', 'Korean', 'demonic', 'tedious', 'political', 'rid', 'inventive', 'atrocious', 'portrayed', 'guilty', 'bring', 'German', 'rental', 'plot', 'consistent', 'individual', 'live-action', 'Good', 'sexy', 'comedic', 'practical', 'campy', 'ambiguous', 'wide', 'remote', 'paced', 'idiotic', 'intriguing', 'separate', 'depth', 'tiresome', 'vague', 'crowd', 'social', 'agree', 'finish']\n"
     ]
    }
   ],
   "source": [
    "horror_train_df = toute_genre_data['Horror']['train_data']\n",
    "horror_unique_adjectives = toute_genre_data['Horror']['unique_adjectives']\n",
    "horror_cent_adj = toute_genre_adjective_freq.get('Horror', {}).get('cent_adj', [])\n",
    "horror_cinqcent_adj = toute_genre_adjective_freq.get('Horror', {}).get('cinqcent_adj', [])\n",
    "print(horror_train_df)\n",
    "print(horror_cent_adj)\n",
    "print(horror_cinqcent_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                mot_adj  note\n",
      "8365  [good, slow, difficult, 10-15, many, unnecessa...   6.0\n",
      "3463  [English, different, psychological, real, open...   8.0\n",
      "1477  [three-week, last, lucky, thought, great, char...  10.0\n",
      "7559  [Luckily, ripped, many, actual, awful, much, u...   1.0\n",
      "1173  [common, original, good, eager, so Regardless,...   8.0\n",
      "...                                                 ...   ...\n",
      "5734  [surprising, simple, predictable, fantastic, e...   7.0\n",
      "5191  [walked, certain, bad, bad, actual, whole, fac...   2.0\n",
      "5390                  [cool, daughter, good, new, like]   7.0\n",
      "860        [beautiful, smart, good, grandfather, great]  10.0\n",
      "7270  [fishy, toxic, nuclear, crew, American, sub, m...   4.0\n",
      "\n",
      "[6019 rows x 2 columns]\n",
      "['good', 'great', 'bad', 'much', 'many', 'little', 'best', 'first', 'real', 'new', 'big', 'original', 'main', 'old', 'whole', 'last', 'different', 'least', 'young', 'better', 'funny', 'low', 'sure', 'interesting', 'beautiful', 'nice', 'poor', 'enough', 'special', 'right', 'excellent', 'decent', 'second', 'wrong', 'entire', 'next', 'scary', 'high', 'worth', 'full', 'predictable', 'hard', 'worst', 'true', 'top', 'classic', 'give', 'short', 'terrible', 'small', 'long', 'able', 'believable', 'stupid', 'serious', 'enjoyable', 'ridiculous', 'obvious', 'awful', 'perfect', 'screen', 'slow', 'single', 'several', 'worse', 'usual', 'strong', 'simple', 'overall', 'human', 'average', 'American', 'fine', 'typical', 'final', 'potential', 'due', 'similar', 'fantastic', 'solid', 'huge', 'romantic', 'dead', 'possible', 'horrible', 'difficult', 'cool', 'emotional', 'major', 'clear', 'important', 'complete', 'dark', 'realistic', 'personal', 'lead', 'wonderful', 'shot', 'brilliant', 'modern']\n",
      "['good', 'great', 'bad', 'much', 'many', 'little', 'best', 'first', 'real', 'new', 'big', 'original', 'main', 'old', 'whole', 'last', 'different', 'least', 'young', 'better', 'funny', 'low', 'sure', 'interesting', 'beautiful', 'nice', 'poor', 'enough', 'special', 'right', 'excellent', 'decent', 'second', 'wrong', 'entire', 'next', 'scary', 'high', 'worth', 'full', 'predictable', 'hard', 'worst', 'true', 'top', 'classic', 'give', 'short', 'terrible', 'small', 'long', 'able', 'believable', 'stupid', 'serious', 'enjoyable', 'ridiculous', 'obvious', 'awful', 'perfect', 'screen', 'slow', 'single', 'several', 'worse', 'usual', 'strong', 'simple', 'overall', 'human', 'average', 'American', 'fine', 'typical', 'final', 'potential', 'due', 'similar', 'fantastic', 'solid', 'huge', 'romantic', 'dead', 'possible', 'horrible', 'difficult', 'cool', 'emotional', 'major', 'clear', 'important', 'complete', 'dark', 'realistic', 'personal', 'lead', 'wonderful', 'shot', 'brilliant', 'modern', 'strange', 'weak', 'intense', 'previous', 'happy', 'easy', 'early', 'mysterious', 'general', 'see', 'certain', 'biggest', 'less', 'negative', 'suspense', 'know', 'past', 'black', 'flat', 'amazing', 'entertaining', 'recent', 'unique', 'close', 'cheap', 'psychological', 'actual', 'interested', 'visual', 'effective', 'sexual', 'open', 'favorite', 'female', 'weird', 'memorable', 'disappointed', 'sci-fi', 'fun', 'former', 'social', 'particular', 'deep', ' re', 'hot', 'dull', 'normal', 'live', 'future', 'present', 'want', 'large', 'powerful', 'successful', 'familiar', 'third', 'hilarious', 'honest', 'evil', 'end', 'script', 'awesome', 'unbelievable', 'fair', 'fresh', 'middle', 'incredible', 'Armenian', 'half', 'supernatural', 'smart', 'impossible', 'watch', 'outstanding', 'dramatic', 'total', 'free', 'comic', ' ve', 'bigger', 'like', 'secret', 'narrative', 'positive', 'light', 'likable', 'read', 'crazy', 'local', 'happen', 'complex', 'intelligent', 'wish', 'suspenseful', 'rich', 'famous', 'sad', 'boring', 'impressive', 'necessary', 'white', 'novel', 'flick', 'gorgeous', 'genre', 'various', 'late', 'viewer', 'worthy', 'write', 'understand', 'couple', 'tough', 'basic', 'liked', 'corporate', 'subject', 'natural', 'political', 'unnecessary', 'character', 'psychotic', 'cinematic', 'tense', 'minor', 'common', 'teen', 'wild', 'surprising', 'watchable', 'historical', 'Russian', 'serial', 'latest', 'saw', 'straight', 'thought', 'type', 'willing', 'front', 'mean', 'popular', 'direct', 'acting', 'criminal', 'dumb', 'higher', 'creepy', 'graphic', 'surprised', 'central', 'angry', 'attractive', 'cast', ' The', 'martial', 'pretty', 'aspect', 'standard', 'capable', 'violent', 'horror', 'musical', 'enjoyed', 'love', 'military', 'Good', 'girl', 'mixed', ' ll', 'Italian', 'guess', 'enjoy', 'Julian', 'detective', 'subtle', 'professional', 'loose', 'disappointing', 'dangerous', 'unrealistic', 'pathetic', 'Overall', 'older', 'extra', 'aware', 'sweet', 'regular', 'multiple', 'male', 'French', 'heavy', 'sexy', 'rare', 'follow', 'storyline', 'fight', 'OK', 'greatest', 'watched', 'laughable', 'curious', 'sound', 'forgettable', 'Last', 'British', 'odd', 'extreme', 'unexpected', 'safe', 'come', 'touch', 'creative', 'generic', 'public', 'exciting', 'super', 'independent', 'sleep', 'unknown', 'genuine', 'alive', 'twist', 'sick', 'start', 'constant', 'younger', 'comedic', 'key', 'tale', 'gross', 'else', 'massive', 'erotic', 'passable', 'numerous', 'drunk', 'bizarre', 'likely', 'ten', 'apparent', 'innocent', 'loud', 'wooden', 'moral', 'mental', 'quick', 'spectacular', 'clever', 'nasty', 'tight', 'plausible', 'alien', 'empty', 'fake', 'ready', 'red', 'finish', 'physical', 'directorial', 'cheesy', 'fit', 'unusual', 'tedious', 'plot', 'avoid', 'make', 'Many', 'illogical', 'outside', 'uncomfortable', ' d', 'hear', 'available', 'cold', 'plenty', 'gruesome', 'nuclear', 'religious', 'proper', 'unable', 'individual', 'become', 'glad', 'afraid', 'Sure', 'laugh', 'quiet', 'stylish', 'desperate', 'humorous', 'gratuitous', 'painful', 'deeper', 'lot', 'popcorn', 'mindless', 'shallow', 'keep', 'pleasant', 'brief', 'pretentious', 'oh', 'absolute', 'over-the-top', 'significant', 'stereotypical', 'thin', 'opposite', 'dry', 'table', 'artistic', 'Australian', 'international', 'stuck', 'underrated', 'charismatic', 'greater', 'guy', 'current', 'facial', 'wide', 'Asian', 'cinema', 'lame', 'involved', 'adequate', 'sensitive', 'agree', 'purpose', 'thriller', 'responsible', 'impressed', 'stop', 'depth', 'ordinary', 'survive', 'finale', 'title', 'limited', 'Indian', 'tremendous', 'sympathetic', 'quite', 'sloppy', 'technical', 'french', 'lazy', 'die', 'initial', 'occasional', 'logical', 'mediocre', 'longer', 'German', 'stronger', 'rest', 'Japanese', 'theatrical', 'dystopian', 'absurd', 'unpredictable', 'genocide', 'Turkish', 'documentary', 'suspect', 'giant', 'exceptional', 'intriguing', 'actress', 'superior', 'find', 'appreciate', 'bunch', 'miss', 'Christian', 'technological', 'act', 'specific', 'nature', 'save', 'meaningful', 'side', 'magical', 'left', 'paced', 'idiotic', 'naked', 'feel', 'excitement', 'fast', 'equal', 'logic', 'hate', 'medical', 'imaginative', 'private', 'crappy', 'acceptable', 'magnificent']\n"
     ]
    }
   ],
   "source": [
    "thriller_train_df = toute_genre_data['Thriller']['train_data']\n",
    "thriller_unique_adjectives = toute_genre_data['Thriller']['unique_adjectives']\n",
    "thriller_cent_adj = toute_genre_adjective_freq.get('Thriller', {}).get('cent_adj', [])\n",
    "thriller_cinqcent_adj = toute_genre_adjective_freq.get('Thriller', {}).get('cinqcent_adj', [])\n",
    "print(thriller_train_df)\n",
    "print(thriller_cent_adj)\n",
    "print(thriller_cinqcent_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                mot_adj  note\n",
      "285   [equal, classic, outcast, medieval, re-enactme...   7.0\n",
      "1304  [good, legendary, celebrate, keep, human, inte...   6.0\n",
      "428   [early, many, well-known, iconic, well-known, ...  10.0\n",
      "1608  [recommend, finish, pulling, sure, future, gre...   9.0\n",
      "757   [promising, great, good, realistic, great, rea...   2.0\n",
      "...                                                 ...   ...\n",
      "1130  [previous, new, classic, fairy, Maldonian, sou...   9.0\n",
      "1294  [little, many, animated, innovative, deep, who...   9.0\n",
      "860   [green, nonsense, whole, shot, talent, latest,...   1.0\n",
      "1459  [restorer, boy-friend, flat, stupid, nice, fro...   6.0\n",
      "1126  [best, great, bad, good, best, great, twist, c...   8.0\n",
      "\n",
      "[1153 rows x 2 columns]\n",
      "['good', 'bad', 'great', 'little', 'much', 'many', 'original', 'best', 'old', 'new', 'first', 'classic', 'whole', 'funny', 'real', 'young', 'big', 'beautiful', 'nice', 'sure', 'least', 'last', 'different', 'better', 'main', 'special', 'fairy', 'terrible', 'hard', 'right', 'wrong', 'musical', 'true', 'interesting', 'worth', 'evil', 'short', 'high', 'happy', 'enough', 'human', 'next', 'small', 'top', 'obvious', 'decent', 'poor', 'second', 'worst', 'enjoyable', 'overall', 'awful', 'live', 'perfect', 'previous', 'wonderful', 'magic', 'dark', 'screen', 'animated', 'comic', 'memorable', 'modern', 'flat', 'disappointed', 'stupid', 'long', 'visual', 'low', 'potential', 'final', 'entire', 'full', 'complete', 'serious', 'able', 'dead', 'amazing', 'actual', 'excellent', 'predictable', 'negative', 'white', 'spectacular', 'give', 'early', 'live-action', 'fine', 'romantic', 'magical', 'tale', 'horrible', 'huge', 'hilarious', 'usual', 'impressive', 'fun', 'several', 'favorite', 'simple']\n",
      "['good', 'bad', 'great', 'little', 'much', 'many', 'original', 'best', 'old', 'new', 'first', 'classic', 'whole', 'funny', 'real', 'young', 'big', 'beautiful', 'nice', 'sure', 'least', 'last', 'different', 'better', 'main', 'special', 'fairy', 'terrible', 'hard', 'right', 'wrong', 'musical', 'true', 'interesting', 'worth', 'evil', 'short', 'high', 'happy', 'enough', 'human', 'next', 'small', 'top', 'obvious', 'decent', 'poor', 'second', 'worst', 'enjoyable', 'overall', 'awful', 'live', 'perfect', 'previous', 'wonderful', 'magic', 'dark', 'screen', 'animated', 'comic', 'memorable', 'modern', 'flat', 'disappointed', 'stupid', 'long', 'visual', 'low', 'potential', 'final', 'entire', 'full', 'complete', 'serious', 'able', 'dead', 'amazing', 'actual', 'excellent', 'predictable', 'negative', 'white', 'spectacular', 'give', 'early', 'live-action', 'fine', 'romantic', 'magical', 'tale', 'horrible', 'huge', 'hilarious', 'usual', 'impressive', 'fun', 'several', 'favorite', 'simple', 'fantastic', 'American', 'average', 'clear', 'major', 'due', 'future', 'free', 'girl', 'believable', 'gorgeous', 'familiar', 'possible', 'unnecessary', 'weak', 'famous', 'successful', 'younger', 'fairytale', 'past', 'third', 'large', 'alive', ' re', 'difficult', 'odd', 'forgettable', 'cute', 'mean', 'know', 'biggest', 'thought', 'single', 'likable', 'certain', 'important', 'similar', 'ridiculous', 'latest', 'realistic', 'particular', 'strong', 'servant', 'sound', 'incredible', 'script', 'classical', 'cheap', 'see', 'natural', 'open', 'favourite', 'read', 'come', 'quiet', 'close', 'over-the-top', 'worse', 'slow', 'emotional', 'sweet', 'general', 'easy', 'shot', 'wish', 'handsome', 'subject', ' ll', 'dull', 'various', 'bigger', 'flick', 'watched', 'common', 'typical', 'light', 'direct', 'fresh', 'older', 'positive', 'surprising', 'British', 'likely', 'Many', 'popular', 'understand', 'forest', 'worthy', 'green', 'strange', 'rich', 'wide', 'personal', 'entertaining', 'animal', 'late', 'twist', 'less', 'basic', 'wasted', 'smart', 'brilliant', 'eternal', 'suitable', 'social', 'nasty', 'left', 'narrative', 'hot', 'fake', 'queen', 'want', 'Happy', 'proper', 'solid', 'ready', 'deep', 'critic', 'central', 'terrific', 'interested', 'powerful', 'watchable', 'ten', 'bizarre', 'type', 'total', 'scary', 'song', 'complex', 'genuine', 'comedic', 'vibrant', 'castle', 'sad', 'touch', 'boring', 'dubious', 'stereotypical', 'plot', 'like', 'boy', 'impressed', 'novel', 'willing', 'safe', 'pointless', 'unusual', 'iconic', 'smile', 'facial', 'greatest', 'low-budget', 'creative', 'awesome', 'clever', 'humorous', 'nostalgic', 'harsh', 'straight', 'necessary', 'heavy', 'vague', 'princess', 'endless', 'recent', 'bright', 'unique', 'Japanese', 'present', 'tried', 'OK', 'brutal', 'empty', 'till', 'head', 'dramatic', 'wooden', 'skin', 'colorful', 'fantasy', 'character', 'box', 'glad', 'tragic', 'mad', ' ve', 'loud', 'title', 'tower', 'military', 'key', 'Sure', 'kiss', 'female', 'bit', 'magnificent', 'cast', 'enjoyed', 'bunch', 'make-up', 'comical', 'witty', 'liked', 'disappointing', 'half', 'tired', 'available', 'appropriate', 'specific', 'outside', 'aware', 'storyline', 'sci-fi', 'level', 'cool', 'attractive', 'corporate', 'uninspired', 'well-known', 'silly', 'lavish', 'marvelous', 'lazy', 'laughable', 'end', 'rest', 'middle', 'detailed', 'political', 'mysterious', 'weird', 'giant', 'cathedral', 'hideous', 'innocent', 'essential', 'lovable', 'afraid', 'inordinate', 'poignant', 'stick', 'unrealistic', 'Indian', 'keep', 'agree', 'surprised', 'pleasant', 'former', 'make', 'unexpected', 'enjoy', 'lead', 'fit', 'forth', 'dangerous', 'expensive', 'historical', 'happen', 'else', 'grand', 'laugh', 'black', 'charismatic', '21st', 'silent', 'superior', 'form', 'dumb', 'cinematic', 'watch', 'minor', 'humor', 'universe', 'technical', 'pathetic', 'sorry', 'adorable', 'precious', 'generic', 'mechanical', 'faithful', 'crappy', 'Overall', 'lush', 'funniest', 'tiny', 'inappropriate', 'absurd', 'alone', 'traditional', 'tough', 'unnatural', 'quick', 'write', 'initial', 'noble', 'asleep', 'delightful', 'male', 'awkward', 'Mumble', 'legendary', 'parallel', 'theatrical', 'flesh', 'rushed', 'recommend', 'adventurous', 'feeling', 'honest', 'summary', 'rough', 'latter', 'represent', 'guilty', 'blame', 'suppose', 'fast-paced', 'notice', 'sigh', 'forward', 'guess', 'painful', 'weakest', 'played', 'let', 'modest', 'curious', 'pop', 'mixed', 'numerous', '3-D', 'forgive', 'Russian', 'portrayed', 'earth', 'adult', 'brief', 'slapstick', 'respective', 'prime', 'love', 'sudden', 'non-existent', 'actress', 'ambitious', 'appreciate', 'physical', 'spindle', 'bottom', 'unfortunate', 'remarkable', 'easier', 'fifteen', 'portray', 'twisted', 'thrown', 'follow', ' d', 'forced', 'material', 'mess', 'ogre', 'capable', 'red', 'sole', 'portrayal', 'smooth', 'moral', 'start', 'normal', 'native', 'clean', 'french', 'happiness', 'anime', 'Good', 'public', 'thin', 'stop', 'equal', 'edgy', 'beloved', 'shoot', 'scared', 'beautifully If', 'politically-correct', 'college She', 'poorly', 'lament', 'feminist', 'childbearing', 'plenty', 'Siamese', 'fight', 'outstanding', 'sexual', 'reminiscent', 'porn']\n"
     ]
    }
   ],
   "source": [
    "animation_train_df = toute_genre_data['Animation']['train_data']\n",
    "animation_unique_adjectives = toute_genre_data['Animation']['unique_adjectives']\n",
    "animation_cent_adj = toute_genre_adjective_freq.get('Animation', {}).get('cent_adj', [])\n",
    "animation_cinqcent_adj = toute_genre_adjective_freq.get('Animation', {}).get('cinqcent_adj', [])\n",
    "print(animation_train_df)\n",
    "print(animation_cent_adj)\n",
    "print(animation_cinqcent_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 mot_adj  note\n",
      "16025  [interested, dubious, dubious, bad, non-existe...   1.0\n",
      "3576   [dead, big, writer, meet, popular, full, impre...   7.0\n",
      "9863   [close, convey, important, good, able, able, a...   7.0\n",
      "14461  [good, semi-remake, little, easier, good, idea...   2.0\n",
      "5516   [good, semi-remake, little, easier, good, idea...   2.0\n",
      "...                                                  ...   ...\n",
      "11284  [mixed, good, good, fluffy, depth, emotional, ...   7.0\n",
      "11964  [bad, great, whole, complain, whole, good, swe...   7.0\n",
      "5390                            [teen, authentic, right]   4.0\n",
      "860    [infamous, genocide, first, 20th, present, eth...   8.0\n",
      "15795  [root, sociopathic, particular, enjoyable, ove...   8.0\n",
      "\n",
      "[13428 rows x 2 columns]\n",
      "['good', 'great', 'bad', 'much', 'many', 'little', 'best', 'funny', 'real', 'first', 'romantic', 'new', 'original', 'big', 'whole', 'old', 'young', 'beautiful', 'different', 'nice', 'last', 'least', 'main', 'better', 'sure', 'high', 'hard', 'true', 'right', 'second', 'classic', 'screen', 'wrong', 'enough', 'full', 'excellent', 'small', 'perfect', 'predictable', 'interesting', 'long', 'special', 'entire', 'worth', 'poor', 'next', 'able', 'hilarious', 'wonderful', 'low', 'happy', 'emotional', 'worst', 'terrible', 'short', 'decent', 'enjoyable', 'top', 'fine', 'strong', 'Armenian', 'musical', 'overall', 'give', 'serious', 'human', 'modern', 'fantastic', 'believable', 'single', 'novel', 'several', 'negative', 'obvious', 'horrible', 'simple', 'awful', 'stupid', 'important', 'live', 'previous', 'similar', 'American', 'typical', 'possible', 'worse', 'comic', 'usual', 'disappointed', 'major', 'easy', 'weak', 'due', 'final', 'huge', 'likable', 'complete', 'know', 'favorite', 'difficult']\n",
      "['good', 'great', 'bad', 'much', 'many', 'little', 'best', 'funny', 'real', 'first', 'romantic', 'new', 'original', 'big', 'whole', 'old', 'young', 'beautiful', 'different', 'nice', 'last', 'least', 'main', 'better', 'sure', 'high', 'hard', 'true', 'right', 'second', 'classic', 'screen', 'wrong', 'enough', 'full', 'excellent', 'small', 'perfect', 'predictable', 'interesting', 'long', 'special', 'entire', 'worth', 'poor', 'next', 'able', 'hilarious', 'wonderful', 'low', 'happy', 'emotional', 'worst', 'terrible', 'short', 'decent', 'enjoyable', 'top', 'fine', 'strong', 'Armenian', 'musical', 'overall', 'give', 'serious', 'human', 'modern', 'fantastic', 'believable', 'single', 'novel', 'several', 'negative', 'obvious', 'horrible', 'simple', 'awful', 'stupid', 'important', 'live', 'previous', 'similar', 'American', 'typical', 'possible', 'worse', 'comic', 'usual', 'disappointed', 'major', 'easy', 'weak', 'due', 'final', 'huge', 'likable', 'complete', 'know', 'favorite', 'difficult', 'female', 'realistic', 'brilliant', 'average', 'personal', 'ridiculous', 'rich', 'certain', 'lead', 'comedic', 'gorgeous', 'teen', 'fresh', 'girl', 'early', 'wish', 'clear', 'flat', 'amazing', 'deep', 'less', 'general', 'shot', 'dramatic', ' re', 'sad', 'familiar', 'watch', 'potential', 'slow', 'see', 'sweet', 'unique', 'fun', 'older', 'actual', 'hot', 'white', 'light', 'surprising', 'successful', 'famous', 'interested', 'popular', 'sexual', 'recent', 'third', 'cool', 'dead', 'open', 'script', 'historical', 'present', 'black', 'large', 'visual', 'memorable', 'entertaining', 'read', 'Indian', 'mean', 'laugh', 'positive', 'past', 'impressive', 'future', 'unnecessary', 'particular', 'former', 'biggest', 'total', 'close', 'strange', 'boring', 'middle', 'dark', 'solid', 'attractive', 'want', 'like', 'powerful', 'incredible', 'liked', 'half', 'cinematic', 'honest', 'saw', 'evil', 'free', 'fairy', 'smart', 'subject', 'cast', 'touch', 'Russian', 'normal', 'late', 'glad', 'younger', 'British', 'enjoy', 'male', 'natural', 'scary', 'cute', 'social', 'couple', 'end', 'straight', 'character', 'weird', 'love', 'dull', 'understand', 'magic', 'come', 'worthy', 'public', 'cheap', ' ve', 'creative', 'empty', ' ll', 'various', 'fake', 'outstanding', 'impossible', 'local', 'genuine', 'political', 'basic', 'genocide', 'moral', 'quiet', 'loud', 'odd', 'fantasy', 'central', 'tragic', 'narrative', 'Last', 'viewer', 'greatest', 'awesome', 'common', 'Turkish', 'bigger', 'pathetic', 'direct', 'intense', 'shallow', 'tale', 'watchable', 'type', 'write', 'ready', 'effective', 'witty', 'unbelievable', 'surprised', 'painful', 'innocent', 'French', 'thought', 'OK', 'humorous', 'complex', 'crazy', 'funniest', 'bizarre', 'Overall', 'fair', 'unexpected', 'willing', 'smile', 'physical', 'unrealistic', 'latest', 'material', 'over-the-top', 'ten', 'angry', 'terrific', 'likely', 'intelligent', 'commercial', 'sound', 'flick', 'front', 'standard', 'follow', 'alive', 'actress', 'minor', 'critical', 'disappointing', 'tough', 'individual', 'religious', 'happen', ' d', 'magical', 'subtle', 'enjoyed', 'favourite', 'Many', 'married', 'professional', 'dumb', 'constant', 'secret', 'pleasant', 'clever', 'watched', 'handsome', 'forgettable', 'fabulous', 'silly', 'rest', 'title', 'depth', 'else', 'rare', 'plot', 'necessary', 'erotic', 'acting', 'delightful', 'brief', 'plenty', 'military', 'wild', 'guess', 'pretty', 'boy', 'critic', 'opposite', 'key', 'deeper', 'facial', 'latter', 'exceptional', 'wealthy', 'mysterious', 'pretentious', 'heavy', 'thin', 'wasted', 'song', 'mixed', 'nasty', 'loose', 'extra', 'Good', 'guy', 'suitable', 'capable', 'comical', 'aware', 'Italian', 'curious', 'directorial', 'left', 'fit', 'sick', 'guilty', 'lovable', 'comedy', 'become', 'act', 'spectacular', 'impressed', 'laughable', 'hear', 'unusual', 'generic', 'afraid', 'current', 'sexy', 'live-action', 'cinema', 'well-known', 'start', 'marvelous', 'medical', 'quick', 'faithful', 'sleep', 'tried', 'cold', 'finish', 'confident', 'initial', 'dry', 'awkward', 'stereotypical', 'superficial', 'vast', 'theatrical', 'safe', 'legendary', 'bunch', 'higher', 'quirky', 'massive', 'sorry', 'drunk', 'ordinary', 'warm', 'aristocratic', 'corporate', 'sympathetic', 'learn', 'contemporary', 'bring', 'make', 'twist', 'absurd', 'chick', 'remarkable', 'sappy', 'uncomfortable', 'sensitive', 'tired', 'lame', 'charismatic', 'gross', 'low-budget', 'purpose', 'fairytale', 'offensive', 'soft', 'porn', 'adorable', 'artistic', 'expect', 'lucky', 'desperate', 'hate', 'independent', 'suspense', 'documentary', 'unfunny', 'supernatural', 'lot', 'specific', 'talent', 'universe', 'Christian', 'greater', 'mediocre', 'numerous', 'ensemble', 'bit', 'vague', 'pick', 'regular', 'stick', 'push', 'fellow', 'dubious', 'tedious', 'wooden', 'trite', 'pointless', 'false', 'recommend', 'sci-fi', 'clean', 'genre', 'obnoxious', 'summary', 'outside', 'poignant', 'appreciate', 'Sure', 'graphic', 'scene', 'storyline', 'superior', 'prime', 'portrayal', 'available', 'slapstick', 'skin', 'agree', 'acceptable', 'rough', 'Julian', 'multiple', 'bright', 'earlier', 'appropriate', 'comfortable', 'underwater', 'useless', 'suppose', 'generous', 'harsh', 'involved']\n"
     ]
    }
   ],
   "source": [
    "romance_train_df = toute_genre_data['Romance']['train_data']\n",
    "romance_unique_adjectives = toute_genre_data['Romance']['unique_adjectives']\n",
    "romance_cent_adj = toute_genre_adjective_freq.get('Romance', {}).get('cent_adj', [])\n",
    "romance_cinqcent_adj = toute_genre_adjective_freq.get('Romance', {}).get('cinqcent_adj', [])\n",
    "print(romance_train_df)\n",
    "print(romance_cent_adj)\n",
    "print(romance_cinqcent_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   binary_note  nombre_mot  nombre_positive  nombre_negative  positive_ratio  \\\n",
      "0            0         7.0              1.0              3.0        0.142857   \n",
      "1            0        35.0             11.0              8.0        0.314286   \n",
      "2            0         2.0              2.0              0.0        1.000000   \n",
      "3            0        12.0              4.0              6.0        0.333333   \n",
      "4            0        14.0              3.0              2.0        0.214286   \n",
      "\n",
      "   negative_ratio  \n",
      "0        0.428571  \n",
      "1        0.228571  \n",
      "2        0.000000  \n",
      "3        0.500000  \n",
      "4        0.142857  \n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:            binary_note   Log-Likelihood:                -10395.\n",
      "Model:                   OrderedModel   AIC:                         2.080e+04\n",
      "Method:            Maximum Likelihood   BIC:                         2.082e+04\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        18:11:24                                         \n",
      "No. Observations:               15909                                         \n",
      "Df Residuals:                   15906                                         \n",
      "Df Model:                           2                                         \n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "positive_ratio     2.0498      0.125     16.343      0.000       1.804       2.296\n",
      "negative_ratio    -3.2909      0.150    -21.987      0.000      -3.584      -2.998\n",
      "0/1               -0.0223      0.057     -0.390      0.697      -0.135       0.090\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Faire le modèle logistique entre des notes et des ratios postives et négatives par chaque comentaires \n",
    "nombre_comedy_df = comedy_df.apply(calculate_sentiments, axis=1)\n",
    "median_comedy=comedy_df['note'].median()\n",
    "comedy_df['binary_note'] = binarize_by_median(comedy_df, 'note', median_comedy)\n",
    "nombre_comedy_df=pd.concat([comedy_df[['binary_note']], nombre_comedy_df], axis=1)\n",
    "print(nombre_comedy_df.head())\n",
    "X = nombre_comedy_df[['positive_ratio', 'negative_ratio']]\n",
    "y= nombre_comedy_df['binary_note']\n",
    "model_initial = OrderedModel(y, X, distr='logit')\n",
    "result_nombre_comedy = model_initial.fit(method='bfgs', disp=False)\n",
    "print(result_nombre_comedy.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   binary_note  nombre_mot  nombre_positive  nombre_negative  positive_ratio  \\\n",
      "0            0        13.0              7.0              1.0        0.538462   \n",
      "1            0         8.0              2.0              4.0        0.250000   \n",
      "2            1         9.0              1.0              4.0        0.111111   \n",
      "3            1        13.0              4.0              3.0        0.307692   \n",
      "4            1         8.0              3.0              3.0        0.375000   \n",
      "\n",
      "   negative_ratio  \n",
      "0        0.076923  \n",
      "1        0.500000  \n",
      "2        0.444444  \n",
      "3        0.230769  \n",
      "4        0.375000  \n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:            binary_note   Log-Likelihood:                -3146.1\n",
      "Model:                   OrderedModel   AIC:                             6298.\n",
      "Method:            Maximum Likelihood   BIC:                             6318.\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        18:11:29                                         \n",
      "No. Observations:                4887                                         \n",
      "Df Residuals:                    4884                                         \n",
      "Df Model:                           2                                         \n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "positive_ratio     2.8849      0.251     11.486      0.000       2.393       3.377\n",
      "negative_ratio    -3.2572      0.268    -12.174      0.000      -3.782      -2.733\n",
      "0/1               -0.0098      0.108     -0.090      0.928      -0.222       0.203\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Faire le modèle logistique entre des notes et des ratios postives et négatives par chaque comentaires \n",
    "nombre_horror_df = horror_df.apply(calculate_sentiments, axis=1)\n",
    "median_horror=horror_df['note'].median()\n",
    "horror_df['binary_note'] = binarize_by_median(horror_df, 'note', median_horror)\n",
    "nombre_horror_df=pd.concat([horror_df[['binary_note']], nombre_horror_df], axis=1)\n",
    "print(nombre_horror_df.head())\n",
    "X = nombre_horror_df[['positive_ratio', 'negative_ratio']]\n",
    "y= nombre_horror_df['binary_note']\n",
    "model_initial = OrderedModel(y, X, distr='logit')\n",
    "result_nombre_horror = model_initial.fit(method='bfgs', disp=False)\n",
    "print(result_nombre_horror.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   binary_note  nombre_mot  nombre_positive  nombre_negative  positive_ratio  \\\n",
      "0            0        23.0              8.0              6.0        0.347826   \n",
      "1            0        55.0             22.0             10.0        0.400000   \n",
      "2            1        62.0             16.0              5.0        0.258065   \n",
      "3            1        21.0              8.0              2.0        0.380952   \n",
      "4            0        10.0              1.0              2.0        0.100000   \n",
      "\n",
      "   negative_ratio  \n",
      "0        0.260870  \n",
      "1        0.181818  \n",
      "2        0.080645  \n",
      "3        0.095238  \n",
      "4        0.200000  \n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:            binary_note   Log-Likelihood:                -5547.5\n",
      "Model:                   OrderedModel   AIC:                         1.110e+04\n",
      "Method:            Maximum Likelihood   BIC:                         1.112e+04\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        21:10:10                                         \n",
      "No. Observations:                8599                                         \n",
      "Df Residuals:                    8596                                         \n",
      "Df Model:                           2                                         \n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "positive_ratio     2.2632      0.173     13.111      0.000       1.925       2.602\n",
      "negative_ratio    -3.0766      0.196    -15.702      0.000      -3.461      -2.693\n",
      "0/1               -0.2309      0.077     -3.009      0.003      -0.381      -0.080\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Faire le modèle logistique entre des notes et des ratios postives et négatives par chaque comentaires \n",
    "nombre_thriller_df = thriller_df.apply(calculate_sentiments, axis=1)\n",
    "median_thriller=thriller_df['note'].median()\n",
    "thriller_df['binary_note'] = binarize_by_median(thriller_df, 'note', median_thriller)\n",
    "nombre_thriller_df=pd.concat([thriller_df[['binary_note']], nombre_thriller_df], axis=1)\n",
    "print(nombre_thriller_df.head())\n",
    "X = nombre_thriller_df[['positive_ratio', 'negative_ratio']]\n",
    "y= nombre_thriller_df['binary_note']\n",
    "model_initial = OrderedModel(y, X, distr='logit')\n",
    "result_nombre_thriller= model_initial.fit(method='bfgs', disp=False)\n",
    "print(result_nombre_thriller.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   binary_note  nombre_mot  nombre_positive  nombre_negative  positive_ratio  \\\n",
      "0            0         7.0              1.0              3.0        0.142857   \n",
      "1            0        35.0             11.0              8.0        0.314286   \n",
      "2            0         2.0              2.0              0.0        1.000000   \n",
      "3            0        12.0              4.0              6.0        0.333333   \n",
      "4            0        14.0              3.0              2.0        0.214286   \n",
      "\n",
      "   negative_ratio  \n",
      "0        0.428571  \n",
      "1        0.228571  \n",
      "2        0.000000  \n",
      "3        0.500000  \n",
      "4        0.142857  \n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:            binary_note   Log-Likelihood:                -12357.\n",
      "Model:                   OrderedModel   AIC:                         2.472e+04\n",
      "Method:            Maximum Likelihood   BIC:                         2.474e+04\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        21:14:11                                         \n",
      "No. Observations:               19184                                         \n",
      "Df Residuals:                   19181                                         \n",
      "Df Model:                           2                                         \n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "positive_ratio     1.6771      0.118     14.181      0.000       1.445       1.909\n",
      "negative_ratio    -3.5454      0.134    -26.362      0.000      -3.809      -3.282\n",
      "0/1               -0.5055      0.053     -9.453      0.000      -0.610      -0.401\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Faire le modèle logistique entre des notes et des ratios postives et négatives par chaque comentaires \n",
    "nombre_romance_df = romance_df.apply(calculate_sentiments, axis=1)\n",
    "median_romance=romance_df['note'].median()\n",
    "romance_df['binary_note'] = binarize_by_median(romance_df, 'note', median_romance)\n",
    "nombre_romance_df=pd.concat([romance_df[['binary_note']], nombre_romance_df], axis=1)\n",
    "print(nombre_romance_df.head())\n",
    "X = nombre_romance_df[['positive_ratio', 'negative_ratio']]\n",
    "y= nombre_romance_df['binary_note']\n",
    "model_initial = OrderedModel(y, X, distr='logit')\n",
    "result_nombre_romance= model_initial.fit(method='bfgs', disp=False)\n",
    "print(result_nombre_romance.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   binary_note  nombre_mot  nombre_positive  nombre_negative  positive_ratio  \\\n",
      "0            0         9.0              0.0              3.0        0.000000   \n",
      "1            0        32.0              7.0              8.0        0.218750   \n",
      "2            0        13.0              3.0              3.0        0.230769   \n",
      "3            0         9.0              3.0              4.0        0.333333   \n",
      "4            1        59.0             17.0             18.0        0.288136   \n",
      "\n",
      "   negative_ratio  \n",
      "0        0.333333  \n",
      "1        0.250000  \n",
      "2        0.230769  \n",
      "3        0.444444  \n",
      "4        0.305085  \n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:            binary_note   Log-Likelihood:                -996.36\n",
      "Model:                   OrderedModel   AIC:                             1999.\n",
      "Method:            Maximum Likelihood   BIC:                             2015.\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        21:19:16                                         \n",
      "No. Observations:                1648                                         \n",
      "Df Residuals:                    1645                                         \n",
      "Df Model:                           2                                         \n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "positive_ratio     3.5682      0.445      8.024      0.000       2.697       4.440\n",
      "negative_ratio    -4.3503      0.471     -9.231      0.000      -5.274      -3.427\n",
      "0/1               -0.0191      0.192     -0.099      0.921      -0.396       0.358\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Faire le modèle logistique entre des notes et des ratios postives et négatives par chaque comentaires \n",
    "nombre_animation_df = animation_df.apply(calculate_sentiments, axis=1)\n",
    "median_animation=animation_df['note'].median()\n",
    "animation_df['binary_note'] = binarize_by_median(animation_df, 'note', median_animation)\n",
    "nombre_animation_df=pd.concat([animation_df[['binary_note']], nombre_animation_df], axis=1)\n",
    "print(nombre_animation_df.head())\n",
    "X = nombre_animation_df[['positive_ratio', 'negative_ratio']]\n",
    "y= nombre_animation_df['binary_note']\n",
    "model_initial = OrderedModel(y, X, distr='logit')\n",
    "result_nombre_animation= model_initial.fit(method='bfgs', disp=False)\n",
    "print(result_nombre_animation.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       good  great  funny  much    bad  many  little  best  romantic    new  \\\n",
      "10806  0.00    0.0    0.5   0.0  0.000   0.0   0.000  0.00       0.0  0.375   \n",
      "10784  0.00    0.0    0.0   0.0 -0.625   0.0  -0.375  0.00       0.0  0.000   \n",
      "11323  0.75    0.0    0.0   0.0  0.000   0.0  -0.375  0.00       0.0  0.375   \n",
      "10715  0.00    0.0    0.0   0.0  0.000   0.0   0.000  0.00       0.0  0.000   \n",
      "11452  0.00    0.0    0.5   0.0 -0.625   0.0   0.000  0.00       0.0  0.000   \n",
      "...     ...    ...    ...   ...    ...   ...     ...   ...       ...    ...   \n",
      "13418  0.75    0.0    0.0   0.0  0.000   0.0   0.000  0.75       0.0  0.000   \n",
      "5390   0.75    0.0    0.5   0.0  0.000   0.0   0.000  0.00       0.0  0.000   \n",
      "860    0.75    0.0    0.0   0.0  0.000   0.0  -0.375  0.75       0.0  0.000   \n",
      "15795  0.75    0.0    0.5   0.0 -0.625   0.0  -0.375  0.00       0.0  0.000   \n",
      "7270   0.75    0.0    0.5   0.0 -0.625   0.0  -0.375  0.00       0.0  0.000   \n",
      "\n",
      "       ...  sweet  important  disappointed  average  human  early  female  \\\n",
      "10806  ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "10784  ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "11323  ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "10715  ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "11452  ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "...    ...    ...        ...           ...      ...    ...    ...     ...   \n",
      "13418  ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "5390   ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "860    ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "15795  ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "7270   ...    0.0        0.0           0.0      0.0    0.0    0.0     0.0   \n",
      "\n",
      "       due  see  major  \n",
      "10806  0.0  0.0    0.0  \n",
      "10784  0.0  0.0    0.0  \n",
      "11323  0.0  0.0    0.0  \n",
      "10715  0.0  0.0    0.0  \n",
      "11452  0.0  0.0    0.0  \n",
      "...    ...  ...    ...  \n",
      "13418  0.0  0.0    0.0  \n",
      "5390   0.0  0.0    0.0  \n",
      "860    0.0  0.0    0.0  \n",
      "15795  0.0  0.0    0.0  \n",
      "7270   0.0  0.0    0.0  \n",
      "\n",
      "[11136 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "comedy_adj_sentiment_matrix = create_adj_sentiment_matrix(comedy_train_df,comedy_cent_adj )\n",
    "print(comedy_adj_sentiment_matrix)\n",
    "horror_adj_sentiment_matrix = create_adj_sentiment_matrix(horror_train_df,horror_cent_adj )\n",
    "thriller_adj_sentiment_matrix = create_adj_sentiment_matrix(thriller_train_df,thriller_cent_adj )\n",
    "romance_adj_sentiment_matrix = create_adj_sentiment_matrix(romance_train_df,romance_cent_adj )\n",
    "animation_adj_sentiment_matrix = create_adj_sentiment_matrix(animation_train_df,animation_cent_adj )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       note\n",
      "10806   5.0\n",
      "10784   4.0\n",
      "11323   5.0\n",
      "10715   8.0\n",
      "11452   3.0\n",
      "...     ...\n",
      "13418   7.0\n",
      "5390    6.0\n",
      "860    10.0\n",
      "15795   2.0\n",
      "7270    7.0\n",
      "\n",
      "[11136 rows x 1 columns]\n",
      "       note  good  funny    bad  little  best    new  old    big  whole  ...  \\\n",
      "0         0  0.00    0.5  0.000   0.000  0.00  0.375  0.0  0.000    0.0  ...   \n",
      "1         0  0.00    0.0 -0.625  -0.375  0.00  0.000  0.0  0.125    0.0  ...   \n",
      "2         0  0.75    0.0  0.000  -0.375  0.00  0.375  0.0  0.000    0.0  ...   \n",
      "3         1  0.00    0.0  0.000   0.000  0.00  0.000  0.0  0.000    0.0  ...   \n",
      "4         0  0.00    0.5 -0.625   0.000  0.00  0.000  0.0  0.000    0.0  ...   \n",
      "...     ...   ...    ...    ...     ...   ...    ...  ...    ...    ...  ...   \n",
      "11126     0  0.75    0.0  0.000   0.000  0.75  0.000  0.0  0.000    0.0  ...   \n",
      "11127     0  0.00    0.0  0.000   0.000  0.00  0.000  0.0  0.125    0.0  ...   \n",
      "11128     0  0.00    0.0  0.000  -0.375  0.75  0.375  0.0  0.125    0.0  ...   \n",
      "11130     1  0.75    0.5  0.000   0.000  0.00  0.000  0.0  0.000    0.0  ...   \n",
      "11135     1  0.75    0.5 -0.625  -0.375  0.00  0.000  0.0  0.000    0.0  ...   \n",
      "\n",
      "       easy  fantastic  obvious  teen  modern  less  important  disappointed  \\\n",
      "0       0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "1       0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "2       0.0        0.0      0.0   0.0     0.0  -0.5        0.0           0.0   \n",
      "3       0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "4       0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "...     ...        ...      ...   ...     ...   ...        ...           ...   \n",
      "11126   0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "11127   0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "11128   0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "11130   0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "11135   0.0        0.0      0.0   0.0     0.0   0.0        0.0           0.0   \n",
      "\n",
      "       major  total_sentiment  \n",
      "0        0.0            1.625  \n",
      "1        0.0           -0.125  \n",
      "2        0.0           -0.375  \n",
      "3        0.0           -1.125  \n",
      "4        0.0           -1.625  \n",
      "...      ...              ...  \n",
      "11126    0.0            2.750  \n",
      "11127    0.0           -0.625  \n",
      "11128    0.0            1.875  \n",
      "11130    0.0            3.000  \n",
      "11135    0.0            0.500  \n",
      "\n",
      "[6497 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combinaison la matrice de comdy_adj pour prépare la modélisation\n",
    "print(comedy_train_df [['note']])\n",
    "comedy_train_df.reset_index(drop=True, inplace=True)\n",
    "comedy_adj_sentiment_matrix.reset_index(drop=True, inplace=True)\n",
    "comedy_adj = pd.concat([comedy_train_df [['note']], comedy_adj_sentiment_matrix], axis=1)\n",
    "comedy_adj['total_sentiment'] = comedy_adj_sentiment_matrix.sum(axis=1)\n",
    "comedy_adj = comedy_adj.loc[:, (comedy_adj != 0).any(axis=0)]\n",
    "comedy_adj = comedy_adj[comedy_adj['total_sentiment'] != 0]\n",
    "comedy_adj = comedy_adj.dropna(subset=['total_sentiment'])\n",
    "comedy_adj = comedy_adj.drop_duplicates()\n",
    "comedy_adj = comedy_adj.apply(pd.to_numeric, errors='coerce')\n",
    "median_comedy=comedy_df['note'].median()\n",
    "comedy_adj['note'] = binarize_by_median(comedy_adj, 'note', median_comedy)\n",
    "print(comedy_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                   note   Log-Likelihood:                -3830.4\n",
      "Model:                   OrderedModel   AIC:                             7791.\n",
      "Method:            Maximum Likelihood   BIC:                             8231.\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        22:03:08                                         \n",
      "No. Observations:                6497                                         \n",
      "Df Residuals:                    6432                                         \n",
      "Df Model:                          64                                         \n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "good             0.0998      0.075      1.322      0.186      -0.048       0.248\n",
      "funny            0.3645      0.126      2.903      0.004       0.118       0.611\n",
      "bad              1.0143      0.113      9.009      0.000       0.794       1.235\n",
      "little           0.0777      0.188      0.414      0.679      -0.290       0.446\n",
      "best             0.1680      0.094      1.780      0.075      -0.017       0.353\n",
      "new             -0.1178      0.231     -0.510      0.610      -0.570       0.335\n",
      "old             -0.6632      0.222     -2.986      0.003      -1.099      -0.228\n",
      "big              0.2427      0.664      0.365      0.715      -1.059       1.545\n",
      "whole           -0.4227      0.699     -0.605      0.545      -1.792       0.947\n",
      "different        0.5392      0.146      3.692      0.000       0.253       0.825\n",
      "nice             0.1497      0.108      1.385      0.166      -0.062       0.362\n",
      "last            -0.6755      0.384     -1.761      0.078      -1.427       0.076\n",
      "better          -0.2400      0.109     -2.199      0.028      -0.454      -0.026\n",
      "sure             0.2511      0.394      0.638      0.523      -0.520       1.022\n",
      "high             1.6555      0.804      2.060      0.039       0.080       3.231\n",
      "hilarious       -7.8217      0.854     -9.158      0.000      -9.496      -6.148\n",
      "main            -0.6862      0.262     -2.620      0.009      -1.200      -0.173\n",
      "hard             0.2648      0.137      1.932      0.053      -0.004       0.533\n",
      "beautiful        0.3933      0.148      2.654      0.008       0.103       0.684\n",
      "wrong            0.4176      0.177      2.353      0.019       0.070       0.765\n",
      "true             2.3692      0.858      2.763      0.006       0.688       4.050\n",
      "perfect          2.1082      0.245      8.619      0.000       1.629       2.588\n",
      "enough          -2.2164      0.905     -2.448      0.014      -3.991      -0.442\n",
      "classic          1.2877      0.315      4.087      0.000       0.670       1.905\n",
      "happy            0.6914      0.145      4.753      0.000       0.406       0.977\n",
      "enjoyable        1.7057      0.450      3.793      0.000       0.824       2.587\n",
      "small            0.0152      0.325      0.047      0.963      -0.621       0.651\n",
      "excellent        1.0383      0.130      7.973      0.000       0.783       1.294\n",
      "worth            0.5887      0.471      1.249      0.212      -0.335       1.512\n",
      "serious          1.4281      0.964      1.481      0.139      -0.461       3.318\n",
      "able             0.5914      0.991      0.597      0.551      -1.351       2.534\n",
      "worst            2.5398      0.310      8.195      0.000       1.932       3.147\n",
      "wonderful        1.3306      0.183      7.267      0.000       0.972       1.689\n",
      "typical         -1.4823      1.026     -1.445      0.148      -3.492       0.528\n",
      "stupid           0.5849      0.178      3.278      0.001       0.235       0.935\n",
      "comic            0.4771      0.258      1.851      0.064      -0.028       0.982\n",
      "interesting     -0.4798      0.333     -1.443      0.149      -1.132       0.172\n",
      "strong           0.0487      0.272      0.179      0.858      -0.485       0.583\n",
      "decent          -0.9448      0.166     -5.692      0.000      -1.270      -0.619\n",
      "poor             1.2676      0.199      6.386      0.000       0.879       1.657\n",
      "low             -1.1807      0.527     -2.239      0.025      -2.214      -0.147\n",
      "fine            -0.2912      0.364     -0.801      0.423      -1.004       0.422\n",
      "simple          -1.9699      0.584     -3.375      0.001      -3.114      -0.826\n",
      "usual           -0.0281      1.079     -0.026      0.979      -2.142       2.086\n",
      "worse            1.4572      0.240      6.068      0.000       0.987       1.928\n",
      "likable          0.2993      0.580      0.516      0.606      -0.838       1.436\n",
      "several          0.1159      0.286      0.405      0.686      -0.445       0.677\n",
      "emotional        0.3450      0.405      0.852      0.394      -0.449       1.139\n",
      "terrible         2.4754      0.305      8.118      0.000       1.878       3.073\n",
      "musical          0.2022      0.654      0.309      0.757      -1.080       1.484\n",
      "believable       0.5013      0.231      2.166      0.030       0.048       0.955\n",
      "huge             0.1156      1.184      0.098      0.922      -2.204       2.435\n",
      "ridiculous       1.5631      0.270      5.796      0.000       1.035       2.092\n",
      "awful            1.1071      0.214      5.176      0.000       0.688       1.526\n",
      "similar          0.2690      1.167      0.230      0.818      -2.019       2.557\n",
      "easy             0.9059      0.395      2.291      0.022       0.131       1.681\n",
      "fantastic        2.8628      0.451      6.347      0.000       1.979       3.747\n",
      "obvious         -0.8274      0.301     -2.746      0.006      -1.418      -0.237\n",
      "teen             2.3461      1.212      1.935      0.053      -0.030       4.723\n",
      "modern          -0.4444      0.634     -0.701      0.483      -1.687       0.798\n",
      "less             0.6797      0.323      2.107      0.035       0.048       1.312\n",
      "important        0.2690      0.187      1.437      0.151      -0.098       0.636\n",
      "disappointed     1.1352      0.348      3.262      0.001       0.453       1.817\n",
      "major           -1.1481      0.270     -4.245      0.000      -1.678      -0.618\n",
      "0/1              0.0956      0.056      1.705      0.088      -0.014       0.205\n",
      "================================================================================\n",
      "7790.712144219044\n"
     ]
    }
   ],
   "source": [
    "#Modèle Logit Ordonnance \n",
    "y = comedy_adj['note']\n",
    "X = comedy_adj.drop(columns=['note','total_sentiment'])\n",
    "model_inital = OrderedModel(y, X, distr='logit')\n",
    "result = model_inital.fit(method='bfgs', disp=False)\n",
    "aic_initiale=result.aic\n",
    "print(result.summary())\n",
    "print(aic_initiale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      note\n",
      "0      9.0\n",
      "1      8.0\n",
      "2      2.0\n",
      "3      2.0\n",
      "4      1.0\n",
      "...    ...\n",
      "3415   1.0\n",
      "3416   6.0\n",
      "3417   1.0\n",
      "3418   1.0\n",
      "3419  10.0\n",
      "\n",
      "[3420 rows x 1 columns]\n",
      "      note  good    bad  little  best    old    new  scary  whole  better  \\\n",
      "0        1  0.00  0.000   0.000  0.00  0.000  0.000  -0.75  0.000   0.875   \n",
      "1        1  0.75  0.000  -0.375  0.75  0.000  0.375   0.00  0.125   0.000   \n",
      "2        0  0.00  0.000  -0.375  0.00  0.000  0.000   0.00  0.000   0.000   \n",
      "3        0  0.00  0.000   0.000  0.00  0.000  0.375   0.00  0.000   0.000   \n",
      "5        1  0.75  0.000  -0.375  0.00  0.000  0.000   0.00  0.000   0.000   \n",
      "...    ...   ...    ...     ...   ...    ...    ...    ...    ...     ...   \n",
      "3401     1  0.00  0.000  -0.375  0.00  0.000  0.000  -0.75  0.000   0.000   \n",
      "3403     1  0.00  0.000  -0.375  0.00  0.000  0.000   0.00  0.000   0.000   \n",
      "3405     0  0.75 -0.625   0.000  0.75  0.000  0.000   0.00  0.000   0.000   \n",
      "3415     0  0.00 -0.625   0.000  0.00  0.375  0.000   0.00  0.125   0.875   \n",
      "3417     0  0.00 -0.625   0.000  0.00  0.000  0.000   0.00  0.000   0.000   \n",
      "\n",
      "      ...  supernatural  clear  potential  cheap  weird  typical  memorable  \\\n",
      "0     ...           0.0    0.0        0.0  -0.25    0.0    0.000        0.0   \n",
      "1     ...           0.0    0.0        0.0   0.00    0.0    0.125        0.0   \n",
      "2     ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "3     ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "5     ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "...   ...           ...    ...        ...    ...    ...      ...        ...   \n",
      "3401  ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "3403  ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "3405  ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "3415  ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "3417  ...           0.0    0.0        0.0   0.00    0.0    0.000        0.0   \n",
      "\n",
      "      weak  happy  total_sentiment  \n",
      "0      0.0    0.0           -0.875  \n",
      "1      0.0    0.0            2.000  \n",
      "2      0.0    0.0            0.250  \n",
      "3      0.0    0.0            0.125  \n",
      "5      0.0    0.0            0.375  \n",
      "...    ...    ...              ...  \n",
      "3401   0.0    0.0           -1.125  \n",
      "3403   0.0    0.0            0.500  \n",
      "3405   0.0    0.0            2.125  \n",
      "3415   0.0    0.0            0.250  \n",
      "3417   0.0    0.0           -1.375  \n",
      "\n",
      "[1818 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combinaison la matrice de horror_adj pour prépare la modélisation\n",
    "print(horror_train_df [['note']])\n",
    "horror_train_df.reset_index(drop=True, inplace=True)\n",
    "horror_adj_sentiment_matrix.reset_index(drop=True, inplace=True)\n",
    "horror_adj = pd.concat([horror_train_df [['note']], horror_adj_sentiment_matrix], axis=1)\n",
    "horror_adj['total_sentiment'] = horror_adj_sentiment_matrix.sum(axis=1)\n",
    "horror_adj = horror_adj.loc[:, (horror_adj != 0).any(axis=0)]\n",
    "horror_adj = horror_adj[horror_adj['total_sentiment'] != 0]\n",
    "horror_adj = horror_adj.dropna(subset=['total_sentiment'])\n",
    "horror_adj = horror_adj.drop_duplicates()\n",
    "horror_adj = horror_adj.apply(pd.to_numeric, errors='coerce')\n",
    "median_horror=horror_df['note'].median()\n",
    "horror_adj['note'] = binarize_by_median(horror_adj, 'note', median_horror)\n",
    "print(horror_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                   note   Log-Likelihood:                -1032.8\n",
      "Model:                   OrderedModel   AIC:                             2198.\n",
      "Method:            Maximum Likelihood   BIC:                             2561.\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        22:05:38                                         \n",
      "No. Observations:                1818                                         \n",
      "Df Residuals:                    1752                                         \n",
      "Df Model:                          65                                         \n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "good             0.1600      0.149      1.077      0.282      -0.131       0.451\n",
      "bad              1.0591      0.201      5.267      0.000       0.665       1.453\n",
      "little          -0.3844      0.380     -1.012      0.312      -1.129       0.360\n",
      "best             0.1077      0.205      0.525      0.600      -0.294       0.510\n",
      "old             -0.6790      0.441     -1.538      0.124      -1.544       0.186\n",
      "new              0.3710      0.437      0.850      0.396      -0.485       1.227\n",
      "scary           -0.0809      0.231     -0.350      0.726      -0.534       0.372\n",
      "whole            1.2453      1.391      0.895      0.371      -1.481       3.972\n",
      "better          -0.5075      0.210     -2.415      0.016      -0.919      -0.096\n",
      "different        0.9346      0.302      3.091      0.002       0.342       1.527\n",
      "big              2.6148      1.445      1.809      0.070      -0.218       5.448\n",
      "last            -1.2732      0.745     -1.709      0.087      -2.734       0.187\n",
      "funny            0.4033      0.420      0.959      0.337      -0.421       1.227\n",
      "main             0.0737      0.530      0.139      0.889      -0.964       1.112\n",
      "low             -1.2870      0.769     -1.675      0.094      -2.793       0.219\n",
      "nice             0.4038      0.232      1.740      0.082      -0.051       0.859\n",
      "beautiful        1.0473      0.333      3.142      0.002       0.394       1.701\n",
      "classic          0.5059      0.604      0.838      0.402      -0.678       1.689\n",
      "worst            2.1847      0.474      4.610      0.000       1.256       3.113\n",
      "sure            -0.4139      0.799     -0.518      0.604      -1.980       1.152\n",
      "enough           0.9230      1.644      0.561      0.575      -2.300       4.146\n",
      "wrong            0.4752      0.353      1.346      0.178      -0.217       1.167\n",
      "terrible         1.1000      0.390      2.820      0.005       0.335       1.865\n",
      "decent          -0.1286      0.255     -0.504      0.614      -0.628       0.371\n",
      "poor             1.6951      0.320      5.300      0.000       1.068       2.322\n",
      "small           -0.3683      0.623     -0.591      0.555      -1.590       0.854\n",
      "true             5.8660      1.937      3.028      0.002       2.069       9.663\n",
      "high            -0.5522      1.652     -0.334      0.738      -3.790       2.686\n",
      "interesting     -0.3750      0.578     -0.649      0.516      -1.507       0.757\n",
      "excellent        1.0886      0.273      3.991      0.000       0.554       1.623\n",
      "worth            0.5784      0.938      0.617      0.538      -1.260       2.417\n",
      "serious          4.1948      2.365      1.774      0.076      -0.440       8.829\n",
      "able             2.7760      1.989      1.395      0.163      -1.123       6.675\n",
      "several         -0.3114      0.511     -0.609      0.542      -1.313       0.690\n",
      "dead             0.8610      0.336      2.562      0.010       0.202       1.520\n",
      "obvious         -0.8053      0.511     -1.577      0.115      -1.807       0.196\n",
      "stupid           1.8838      0.392      4.812      0.000       1.116       2.651\n",
      "awful            1.0060      0.331      3.038      0.002       0.357       1.655\n",
      "hard             0.3080      0.318      0.969      0.332      -0.315       0.931\n",
      "strange          0.6799      1.097      0.620      0.536      -1.471       2.831\n",
      "possible        -1.0503      0.588     -1.785      0.074      -2.204       0.103\n",
      "usual            3.2204      2.342      1.375      0.169      -1.369       7.810\n",
      "strong           1.1419      0.555      2.056      0.040       0.053       2.230\n",
      "enjoyable        4.1517      1.261      3.292      0.001       1.680       6.623\n",
      "ridiculous       1.4784      0.515      2.873      0.004       0.470       2.487\n",
      "horrible         2.0910      0.513      4.072      0.000       1.085       3.097\n",
      "worse            1.1198      0.385      2.905      0.004       0.364       1.875\n",
      "huge            -3.4093      2.201     -1.549      0.121      -7.723       0.905\n",
      "believable       1.4852      0.526      2.821      0.005       0.453       2.517\n",
      "simple          -2.8500      1.199     -2.376      0.017      -5.201      -0.499\n",
      "evil            -0.4273      0.394     -1.083      0.279      -1.200       0.346\n",
      "modern          -0.9383      1.205     -0.779      0.436      -3.300       1.424\n",
      "fine             1.0081      0.795      1.267      0.205      -0.551       2.567\n",
      "perfect          2.6015      0.715      3.638      0.000       1.200       4.003\n",
      "fantastic        3.3380      1.025      3.257      0.001       1.330       5.346\n",
      "similar         -0.1138      2.295     -0.050      0.960      -4.612       4.384\n",
      "supernatural    -0.0162      0.398     -0.041      0.968      -0.795       0.763\n",
      "clear            0.4604      0.621      0.741      0.459      -0.758       1.678\n",
      "potential        3.5244      1.266      2.784      0.005       1.043       6.006\n",
      "cheap            3.6036      1.177      3.061      0.002       1.296       5.911\n",
      "weird           -0.7867      0.415     -1.896      0.058      -1.600       0.027\n",
      "typical          1.5260      2.132      0.716      0.474      -2.652       5.704\n",
      "memorable        0.0067      2.652      0.003      0.998      -5.190       5.204\n",
      "weak             1.2018      0.872      1.378      0.168      -0.507       2.910\n",
      "happy            0.9084      0.449      2.022      0.043       0.028       1.789\n",
      "0/1              0.0033      0.106      0.031      0.975      -0.205       0.212\n",
      "================================================================================\n",
      "2197.5784093156662\n"
     ]
    }
   ],
   "source": [
    "#Modèle Logit Ordonnance pour film horrible \n",
    "y = horror_adj['note']\n",
    "X = horror_adj.drop(columns=['note','total_sentiment'])\n",
    "model_inital = OrderedModel(y, X, distr='logit')\n",
    "result = model_inital.fit(method='bfgs', disp=False)\n",
    "aic_initiale=result.aic\n",
    "print(result.summary())\n",
    "print(aic_initiale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      note\n",
      "0      6.0\n",
      "1      8.0\n",
      "2     10.0\n",
      "3      1.0\n",
      "4      8.0\n",
      "...    ...\n",
      "6014   7.0\n",
      "6015   2.0\n",
      "6016   7.0\n",
      "6017  10.0\n",
      "6018   4.0\n",
      "\n",
      "[6019 rows x 1 columns]\n",
      "      note  good    bad  little  best    new    big   main  old  whole  ...  \\\n",
      "1        1  0.00  0.000   0.000  0.00  0.000  0.000  0.000  0.0  0.000  ...   \n",
      "2        1  0.00  0.000   0.000  0.00  0.000  0.000  0.375  0.0  0.000  ...   \n",
      "3        0  0.00  0.000   0.000  0.00  0.000  0.000  0.000  0.0  0.000  ...   \n",
      "4        1  0.75  0.000  -0.375  0.00  0.000  0.000  0.000  0.0  0.125  ...   \n",
      "5        0  0.75  0.000   0.000  0.00  0.000  0.000  0.000  0.0  0.000  ...   \n",
      "...    ...   ...    ...     ...   ...    ...    ...    ...  ...    ...  ...   \n",
      "6001     1  0.75  0.000  -0.375  0.00  0.000  0.125  0.000  0.0  0.125  ...   \n",
      "6002     0  0.00  0.000  -0.375  0.00  0.000  0.000  0.000  0.0  0.000  ...   \n",
      "6004     1  0.00  0.000   0.000  0.00  0.375  0.125  0.000  0.0  0.000  ...   \n",
      "6010     1  0.00 -0.625   0.000  0.00  0.000  0.000  0.000  0.0  0.000  ...   \n",
      "6018     0  0.75  0.000   0.000  0.75  0.000  0.125  0.375  0.0  0.125  ...   \n",
      "\n",
      "      difficult   cool  emotional  major  clear  important  wonderful  \\\n",
      "1           0.0  0.000      0.000    0.0    0.0        0.0        0.0   \n",
      "2           0.0  0.000      0.000    0.0    0.0        0.0        0.0   \n",
      "3           0.0  0.000      0.000    0.0    0.0        0.0        0.0   \n",
      "4           0.0  0.000      0.000    0.0    0.0        0.0        0.0   \n",
      "5           0.0  0.000      0.000    0.0    0.0        0.0        0.0   \n",
      "...         ...    ...        ...    ...    ...        ...        ...   \n",
      "6001        0.0  0.000      0.375    0.0    0.0        0.0        0.0   \n",
      "6002        0.0  0.125      0.000    0.0    0.0        0.0        0.0   \n",
      "6004        0.0  0.000      0.000    0.0    0.0        0.0        0.0   \n",
      "6010        0.0  0.000      0.375    0.0    0.0        0.0        0.0   \n",
      "6018        0.0  0.000      0.000    0.0    0.0        0.0        0.0   \n",
      "\n",
      "      brilliant  modern  total_sentiment  \n",
      "1           0.0     0.0            0.625  \n",
      "2           0.0     0.0            0.125  \n",
      "3           0.0     0.0            0.500  \n",
      "4           0.0     0.0            1.500  \n",
      "5           0.0     0.0            2.250  \n",
      "...         ...     ...              ...  \n",
      "6001        0.0     0.0            1.500  \n",
      "6002        0.0     0.0            1.000  \n",
      "6004        0.0     0.0           -0.375  \n",
      "6010        0.0     0.0           -0.250  \n",
      "6018        0.0     0.0           -0.500  \n",
      "\n",
      "[3158 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combinaison la matrice de thriller_adj pour prépare la modélisation\n",
    "print(thriller_train_df [['note']])\n",
    "thriller_train_df.reset_index(drop=True, inplace=True)\n",
    "thriller_adj_sentiment_matrix.reset_index(drop=True, inplace=True)\n",
    "thriller_adj = pd.concat([thriller_train_df [['note']], thriller_adj_sentiment_matrix], axis=1)\n",
    "thriller_adj['total_sentiment'] = thriller_adj_sentiment_matrix.sum(axis=1)\n",
    "thriller_adj =thriller_adj.loc[:, (thriller_adj != 0).any(axis=0)]\n",
    "thriller_adj =thriller_adj[thriller_adj['total_sentiment'] != 0]\n",
    "thriller_adj = thriller_adj.dropna(subset=['total_sentiment'])\n",
    "thriller_adj = thriller_adj.drop_duplicates()\n",
    "thriller_adj = thriller_adj.apply(pd.to_numeric, errors='coerce')\n",
    "median_thriller=thriller_df['note'].median()\n",
    "thriller_adj['note'] = binarize_by_median(thriller_adj, 'note', median_thriller)\n",
    "print(thriller_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                   note   Log-Likelihood:                -1773.6\n",
      "Model:                   OrderedModel   AIC:                             3679.\n",
      "Method:            Maximum Likelihood   BIC:                             4079.\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        22:09:21                                         \n",
      "No. Observations:                3158                                         \n",
      "Df Residuals:                    3092                                         \n",
      "Df Model:                          65                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "good            0.4875      0.113      4.325      0.000       0.267       0.708\n",
      "bad             0.7789      0.153      5.077      0.000       0.478       1.080\n",
      "little         -0.3858      0.289     -1.334      0.182      -0.953       0.181\n",
      "best            0.3939      0.150      2.634      0.008       0.101       0.687\n",
      "new             0.4188      0.351      1.193      0.233      -0.269       1.107\n",
      "big             3.6410      1.061      3.433      0.001       1.562       5.720\n",
      "main           -0.2443      0.362     -0.675      0.499      -0.953       0.465\n",
      "old            -0.0115      0.355     -0.032      0.974      -0.707       0.684\n",
      "whole          -0.2327      1.045     -0.223      0.824      -2.280       1.815\n",
      "last           -1.0009      0.546     -1.834      0.067      -2.071       0.069\n",
      "different       1.1523      0.231      4.979      0.000       0.699       1.606\n",
      "better         -0.4374      0.154     -2.845      0.004      -0.739      -0.136\n",
      "funny           0.5085      0.352      1.445      0.148      -0.181       1.198\n",
      "low            -0.7892      0.618     -1.277      0.202      -2.001       0.422\n",
      "sure           -0.5987      0.588     -1.018      0.309      -1.752       0.554\n",
      "interesting    -1.4086      0.401     -3.511      0.000      -2.195      -0.622\n",
      "beautiful       0.6962      0.233      2.993      0.003       0.240       1.152\n",
      "nice            0.2681      0.189      1.422      0.155      -0.101       0.638\n",
      "poor            1.7405      0.245      7.108      0.000       1.261       2.220\n",
      "enough         -0.4090      1.264     -0.324      0.746      -2.886       2.068\n",
      "excellent       0.9082      0.189      4.804      0.000       0.538       1.279\n",
      "decent         -0.1948      0.186     -1.047      0.295      -0.559       0.170\n",
      "wrong           0.2369      0.255      0.929      0.353      -0.263       0.737\n",
      "scary          -0.0394      0.239     -0.165      0.869      -0.508       0.429\n",
      "high            1.7253      1.283      1.344      0.179      -0.790       4.241\n",
      "worth           1.4291      0.678      2.107      0.035       0.099       2.759\n",
      "hard            0.4651      0.218      2.134      0.033       0.038       0.892\n",
      "worst           2.8259      0.422      6.689      0.000       1.998       3.654\n",
      "true            1.6878      1.394      1.210      0.226      -1.045       4.421\n",
      "classic         0.5387      0.486      1.109      0.267      -0.413       1.491\n",
      "terrible        1.2672      0.328      3.869      0.000       0.625       1.909\n",
      "small          -0.0941      0.498     -0.189      0.850      -1.070       0.882\n",
      "able           -1.4059      1.515     -0.928      0.354      -4.376       1.564\n",
      "believable      0.5558      0.342      1.624      0.104      -0.115       1.226\n",
      "stupid          1.7862      0.310      5.766      0.000       1.179       2.393\n",
      "serious         4.7082      1.651      2.852      0.004       1.472       7.944\n",
      "enjoyable       3.8700      0.919      4.211      0.000       2.069       5.671\n",
      "ridiculous      1.3043      0.342      3.817      0.000       0.635       1.974\n",
      "obvious        -0.8992      0.408     -2.206      0.027      -1.698      -0.100\n",
      "awful           1.3690      0.263      5.212      0.000       0.854       1.884\n",
      "perfect         2.0051      0.470      4.271      0.000       1.085       2.925\n",
      "several        -0.5958      0.434     -1.374      0.170      -1.446       0.254\n",
      "worse           0.5036      0.279      1.806      0.071      -0.043       1.050\n",
      "usual           3.4778      1.669      2.084      0.037       0.208       6.748\n",
      "strong          0.8948      0.419      2.137      0.033       0.074       1.716\n",
      "simple         -3.2690      0.910     -3.593      0.000      -5.052      -1.486\n",
      "fine            0.1170      0.533      0.220      0.826      -0.928       1.162\n",
      "typical         2.9116      1.724      1.689      0.091      -0.467       6.291\n",
      "potential       2.1674      0.954      2.273      0.023       0.298       4.036\n",
      "similar        -0.7576      1.672     -0.453      0.650      -4.034       2.519\n",
      "fantastic       4.0064      0.810      4.947      0.000       2.419       5.594\n",
      "solid           0.6682      0.264      2.534      0.011       0.151       1.185\n",
      "huge            0.0128      1.772      0.007      0.994      -3.459       3.485\n",
      "dead            0.3577      0.284      1.260      0.208      -0.199       0.914\n",
      "possible       -0.5531      0.432     -1.280      0.201      -1.400       0.294\n",
      "horrible        1.5349      0.362      4.238      0.000       0.825       2.245\n",
      "difficult      -0.3447      0.345     -0.999      0.318      -1.021       0.332\n",
      "cool            0.3497      1.937      0.181      0.857      -3.447       4.146\n",
      "emotional       1.1577      0.688      1.682      0.092      -0.191       2.506\n",
      "major          -1.4763      0.362     -4.074      0.000      -2.187      -0.766\n",
      "clear          -0.7033      0.437     -1.610      0.107      -1.560       0.153\n",
      "important       0.4352      0.290      1.499      0.134      -0.134       1.004\n",
      "wonderful       1.4704      0.367      4.007      0.000       0.751       2.190\n",
      "brilliant       0.9470      0.314      3.016      0.003       0.332       1.562\n",
      "modern         -0.5792      0.956     -0.606      0.545      -2.453       1.294\n",
      "0/1            -0.0801      0.080     -1.002      0.317      -0.237       0.077\n",
      "===============================================================================\n",
      "3679.261717454585\n"
     ]
    }
   ],
   "source": [
    "#Modèle Logit Ordonnance pour film thriller\n",
    "y = thriller_adj['note']\n",
    "X = thriller_adj.drop(columns=['note','total_sentiment'])\n",
    "model_inital = OrderedModel(y, X, distr='logit')\n",
    "result = model_inital.fit(method='bfgs', disp=False)\n",
    "aic_initiale=result.aic\n",
    "print(result.summary())\n",
    "print(aic_initiale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       note\n",
      "16025   1.0\n",
      "3576    7.0\n",
      "9863    7.0\n",
      "14461   2.0\n",
      "5516    2.0\n",
      "...     ...\n",
      "11284   7.0\n",
      "11964   7.0\n",
      "5390    4.0\n",
      "860     8.0\n",
      "15795   8.0\n",
      "\n",
      "[13428 rows x 1 columns]\n",
      "       note  good    bad  little  best  funny    new    big  whole  old  ...  \\\n",
      "0         0  0.00 -0.625  -0.375  0.75    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "1         1  0.00  0.000   0.000  0.00    0.0  0.000  0.125  0.000  0.0  ...   \n",
      "2         1  0.75  0.000   0.000  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "3         0  0.75  0.000  -0.375  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "5         1  0.75  0.000   0.000  0.00    0.0  0.000  0.000  0.125  0.0  ...   \n",
      "...     ...   ...    ...     ...   ...    ...    ...    ...    ...  ...  ...   \n",
      "13410     0  0.00  0.000   0.000  0.00    0.0  0.375  0.000  0.000  0.0  ...   \n",
      "13413     1  0.75  0.000  -0.375  0.00    0.5  0.000  0.125  0.000  0.0  ...   \n",
      "13416     1  0.75 -0.625  -0.375  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "13420     1  0.00 -0.625   0.000  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "13427     1  0.75 -0.625   0.000  0.00    0.0  0.375  0.000  0.000  0.0  ...   \n",
      "\n",
      "       usual  disappointed  major   easy   weak  huge  likable  favorite  \\\n",
      "0      0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "1      0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "2      0.125           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "3      0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "5      0.000          -0.5  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "...      ...           ...    ...    ...    ...   ...      ...       ...   \n",
      "13410  0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "13413  0.000           0.0  0.000  0.000 -0.375   0.0      0.0       0.0   \n",
      "13416  0.000           0.0  0.625  0.375  0.000   0.0      0.0       0.0   \n",
      "13420  0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "13427  0.125           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "\n",
      "       difficult  total_sentiment  \n",
      "0            0.0           -1.000  \n",
      "1            0.0            0.375  \n",
      "2            0.0            2.250  \n",
      "3            0.0            1.500  \n",
      "5            0.0           -0.125  \n",
      "...          ...              ...  \n",
      "13410        0.0            0.375  \n",
      "13413        0.0            1.000  \n",
      "13416        0.0            1.500  \n",
      "13420        0.0            0.250  \n",
      "13427        0.0            0.875  \n",
      "\n",
      "[6594 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combinaison la matrice de romance_adj pour prépare la modélisation\n",
    "print(romance_train_df [['note']])\n",
    "romance_train_df.reset_index(drop=True, inplace=True)\n",
    "romance_adj_sentiment_matrix.reset_index(drop=True, inplace=True)\n",
    "romance_adj = pd.concat([romance_train_df [['note']], romance_adj_sentiment_matrix], axis=1)\n",
    "romance_adj['total_sentiment'] = romance_adj_sentiment_matrix.sum(axis=1)\n",
    "romance_adj = romance_adj.loc[:, (romance_adj != 0).any(axis=0)]\n",
    "romance_adj = romance_adj[romance_adj['total_sentiment'] != 0]\n",
    "romance_adj = romance_adj.dropna(subset=['total_sentiment'])\n",
    "romance_adj = romance_adj.drop_duplicates()\n",
    "romance_adj = romance_adj.apply(pd.to_numeric, errors='coerce')\n",
    "median_romance=romance_df['note'].median()\n",
    "romance_adj['note'] = binarize_by_median(romance_adj, 'note', median_romance)\n",
    "print(romance_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       note\n",
      "0       1.0\n",
      "1       7.0\n",
      "2       7.0\n",
      "3       2.0\n",
      "4       2.0\n",
      "...     ...\n",
      "13423   7.0\n",
      "13424   7.0\n",
      "13425   4.0\n",
      "13426   8.0\n",
      "13427   8.0\n",
      "\n",
      "[13428 rows x 1 columns]\n",
      "       note  good    bad  little  best  funny    new    big  whole  old  ...  \\\n",
      "0         0  0.00 -0.625  -0.375  0.75    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "1         1  0.00  0.000   0.000  0.00    0.0  0.000  0.125  0.000  0.0  ...   \n",
      "2         1  0.75  0.000   0.000  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "3         0  0.75  0.000  -0.375  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "5         1  0.75  0.000   0.000  0.00    0.0  0.000  0.000  0.125  0.0  ...   \n",
      "...     ...   ...    ...     ...   ...    ...    ...    ...    ...  ...  ...   \n",
      "13410     0  0.00  0.000   0.000  0.00    0.0  0.375  0.000  0.000  0.0  ...   \n",
      "13413     1  0.75  0.000  -0.375  0.00    0.5  0.000  0.125  0.000  0.0  ...   \n",
      "13416     1  0.75 -0.625  -0.375  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "13420     1  0.00 -0.625   0.000  0.00    0.0  0.000  0.000  0.000  0.0  ...   \n",
      "13427     1  0.75 -0.625   0.000  0.00    0.0  0.375  0.000  0.000  0.0  ...   \n",
      "\n",
      "       usual  disappointed  major   easy   weak  huge  likable  favorite  \\\n",
      "0      0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "1      0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "2      0.125           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "3      0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "5      0.000          -0.5  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "...      ...           ...    ...    ...    ...   ...      ...       ...   \n",
      "13410  0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "13413  0.000           0.0  0.000  0.000 -0.375   0.0      0.0       0.0   \n",
      "13416  0.000           0.0  0.625  0.375  0.000   0.0      0.0       0.0   \n",
      "13420  0.000           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "13427  0.125           0.0  0.000  0.000  0.000   0.0      0.0       0.0   \n",
      "\n",
      "       difficult  total_sentiment  \n",
      "0            0.0           -1.000  \n",
      "1            0.0            0.375  \n",
      "2            0.0            2.250  \n",
      "3            0.0            1.500  \n",
      "5            0.0           -0.125  \n",
      "...          ...              ...  \n",
      "13410        0.0            0.375  \n",
      "13413        0.0            1.000  \n",
      "13416        0.0            1.500  \n",
      "13420        0.0            0.250  \n",
      "13427        0.0            0.875  \n",
      "\n",
      "[6594 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combinaison la matrice de romance_adj pour prépare la modélisation\n",
    "print(romance_train_df [['note']])\n",
    "romance_train_df.reset_index(drop=True, inplace=True)\n",
    "romance_adj_sentiment_matrix.reset_index(drop=True, inplace=True)\n",
    "romance_adj = pd.concat([romance_train_df [['note']], romance_adj_sentiment_matrix], axis=1)\n",
    "romance_adj['total_sentiment'] = romance_adj_sentiment_matrix.sum(axis=1)\n",
    "romance_adj = romance_adj.loc[:, (romance_adj != 0).any(axis=0)]\n",
    "romance_adj = romance_adj[romance_adj['total_sentiment'] != 0]\n",
    "romance_adj = romance_adj.dropna(subset=['total_sentiment'])\n",
    "romance_adj = romance_adj.drop_duplicates()\n",
    "romance_adj = romance_adj.apply(pd.to_numeric, errors='coerce')\n",
    "median_romance=romance_df['note'].median()\n",
    "romance_adj['note'] = binarize_by_median(romance_adj, 'note', median_romance)\n",
    "print(romance_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                   note   Log-Likelihood:                -3673.1\n",
      "Model:                   OrderedModel   AIC:                             7484.\n",
      "Method:            Maximum Likelihood   BIC:                             7953.\n",
      "Date:                Wed, 13 Nov 2024                                         \n",
      "Time:                        22:12:56                                         \n",
      "No. Observations:                6594                                         \n",
      "Df Residuals:                    6525                                         \n",
      "Df Model:                          68                                         \n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "good             0.0719      0.078      0.918      0.358      -0.081       0.225\n",
      "bad              0.8425      0.113      7.454      0.000       0.621       1.064\n",
      "little          -0.3359      0.195     -1.723      0.085      -0.718       0.046\n",
      "best             0.5220      0.099      5.268      0.000       0.328       0.716\n",
      "funny            0.3484      0.149      2.339      0.019       0.057       0.640\n",
      "new             -0.0921      0.237     -0.388      0.698      -0.557       0.373\n",
      "big              0.7870      0.690      1.140      0.254      -0.566       2.140\n",
      "whole           -2.0887      0.704     -2.965      0.003      -3.469      -0.708\n",
      "old             -0.7576      0.228     -3.324      0.001      -1.204      -0.311\n",
      "beautiful        0.7600      0.137      5.556      0.000       0.492       1.028\n",
      "different        0.6594      0.150      4.401      0.000       0.366       0.953\n",
      "nice             0.2947      0.118      2.506      0.012       0.064       0.525\n",
      "last            -1.4640      0.393     -3.729      0.000      -2.233      -0.695\n",
      "main            -0.6054      0.255     -2.374      0.018      -1.105      -0.106\n",
      "better          -0.1132      0.113     -1.003      0.316      -0.334       0.108\n",
      "sure             0.1389      0.400      0.347      0.728      -0.645       0.922\n",
      "high             2.1451      0.816      2.627      0.009       0.545       3.745\n",
      "hard             0.3179      0.137      2.322      0.020       0.050       0.586\n",
      "true             2.4186      0.887      2.726      0.006       0.680       4.158\n",
      "classic          1.2250      0.346      3.543      0.000       0.547       1.903\n",
      "wrong            0.2833      0.184      1.539      0.124      -0.078       0.644\n",
      "enough          -1.7904      0.900     -1.989      0.047      -3.555      -0.026\n",
      "excellent        0.9715      0.142      6.819      0.000       0.692       1.251\n",
      "small            0.1155      0.342      0.338      0.736      -0.555       0.786\n",
      "perfect          2.0319      0.283      7.190      0.000       1.478       2.586\n",
      "interesting     -0.6565      0.312     -2.101      0.036      -1.269      -0.044\n",
      "worth            0.9977      0.508      1.963      0.050       0.002       1.994\n",
      "poor             1.5017      0.178      8.439      0.000       1.153       1.850\n",
      "able             0.6154      0.992      0.620      0.535      -1.329       2.560\n",
      "hilarious      -10.0840      1.195     -8.436      0.000     -12.427      -7.741\n",
      "wonderful        1.3245      0.193      6.848      0.000       0.945       1.704\n",
      "low             -1.1400      0.541     -2.106      0.035      -2.201      -0.079\n",
      "happy            0.6848      0.159      4.307      0.000       0.373       0.996\n",
      "emotional        1.2718      0.380      3.347      0.001       0.527       2.017\n",
      "worst            2.6745      0.291      9.175      0.000       2.103       3.246\n",
      "terrible         1.8238      0.262      6.951      0.000       1.310       2.338\n",
      "decent          -0.4241      0.156     -2.716      0.007      -0.730      -0.118\n",
      "enjoyable        3.9102      0.581      6.730      0.000       2.771       5.049\n",
      "fine            -0.1717      0.361     -0.476      0.634      -0.879       0.536\n",
      "strong           0.5104      0.274      1.863      0.062      -0.026       1.047\n",
      "musical         -0.5615      0.615     -0.913      0.361      -1.767       0.644\n",
      "serious          2.4648      1.101      2.239      0.025       0.307       4.622\n",
      "modern          -0.6949      0.607     -1.145      0.252      -1.884       0.494\n",
      "fantastic        3.3278      0.499      6.671      0.000       2.350       4.305\n",
      "believable       0.5418      0.226      2.398      0.016       0.099       0.985\n",
      "novel            0.5244      0.798      0.657      0.511      -1.039       2.088\n",
      "several          0.1783      0.288      0.619      0.536      -0.386       0.743\n",
      "negative        -0.7530      0.217     -3.466      0.001      -1.179      -0.327\n",
      "obvious         -1.4963      0.297     -5.039      0.000      -2.078      -0.914\n",
      "horrible         2.0688      0.287      7.200      0.000       1.506       2.632\n",
      "simple          -1.8084      0.646     -2.798      0.005      -3.075      -0.542\n",
      "awful            1.2085      0.198      6.106      0.000       0.821       1.596\n",
      "stupid           1.0642      0.199      5.350      0.000       0.674       1.454\n",
      "important        0.3191      0.176      1.810      0.070      -0.026       0.665\n",
      "similar          2.7426      1.234      2.223      0.026       0.324       5.161\n",
      "typical          0.1670      1.140      0.147      0.884      -2.067       2.401\n",
      "possible        -0.8192      0.324     -2.525      0.012      -1.455      -0.183\n",
      "worse            1.2026      0.230      5.238      0.000       0.753       1.653\n",
      "comic            0.1214      0.327      0.371      0.710      -0.520       0.762\n",
      "usual            0.7529      1.157      0.651      0.515      -1.514       3.020\n",
      "disappointed     1.3105      0.329      3.983      0.000       0.666       1.955\n",
      "major           -0.6820      0.257     -2.650      0.008      -1.186      -0.178\n",
      "easy             0.7949      0.414      1.919      0.055      -0.017       1.607\n",
      "weak             1.9711      0.438      4.499      0.000       1.112       2.830\n",
      "huge             0.3378      1.213      0.279      0.781      -2.039       2.715\n",
      "likable          0.5685      0.637      0.892      0.372      -0.680       1.817\n",
      "favorite         4.5062      1.405      3.208      0.001       1.753       7.259\n",
      "difficult        0.3983      0.229      1.743      0.081      -0.050       0.846\n",
      "0/1             -0.2284      0.057     -4.035      0.000      -0.339      -0.117\n",
      "================================================================================\n",
      "7484.1549998309765\n"
     ]
    }
   ],
   "source": [
    "#Modèle Logit Ordonnance pour film romantique \n",
    "y = romance_adj['note']\n",
    "X = romance_adj.drop(columns=['note','total_sentiment'])\n",
    "model_inital = OrderedModel(y, X, distr='logit')\n",
    "result = model_inital.fit(method='bfgs', disp=False)\n",
    "aic_initiale=result.aic\n",
    "print(result.summary())\n",
    "print(aic_initiale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      note\n",
      "285    7.0\n",
      "1304   6.0\n",
      "428   10.0\n",
      "1608   9.0\n",
      "757    2.0\n",
      "...    ...\n",
      "1130   9.0\n",
      "1294   9.0\n",
      "860    1.0\n",
      "1459   6.0\n",
      "1126   8.0\n",
      "\n",
      "[1153 rows x 1 columns]\n",
      "      note  good    bad  little  best    old    new  classic  whole  funny  \\\n",
      "0        1  0.75  0.000   0.000  0.00  0.000  0.000    0.375  0.000    0.0   \n",
      "1        1  0.75  0.000  -0.375  0.00  0.000  0.000    0.000  0.000    0.0   \n",
      "2        1  0.00  0.000  -0.375  0.00  0.000  0.375    0.375  0.125    0.0   \n",
      "3        1  0.00  0.000   0.000  0.00  0.000  0.000    0.000  0.000    0.0   \n",
      "4        0  0.75  0.000   0.000  0.00  0.000  0.000    0.000  0.125    0.5   \n",
      "...    ...   ...    ...     ...   ...    ...    ...      ...    ...    ...   \n",
      "1147     1  0.00 -0.625   0.000  0.00  0.000  0.000    0.000  0.000    0.0   \n",
      "1148     1  0.75  0.000  -0.375  0.00  0.375  0.375    0.375  0.125    0.5   \n",
      "1150     0  0.75 -0.625   0.000  0.00  0.375  0.000    0.000  0.125    0.0   \n",
      "1151     1  0.00  0.000   0.000  0.00  0.000  0.000    0.000  0.000    0.5   \n",
      "1152     1  0.75 -0.625   0.000  0.75  0.000  0.000    0.375  0.000    0.0   \n",
      "\n",
      "      ...  magical  horrible  huge  hilarious  usual  impressive  several  \\\n",
      "0     ...      0.0       0.0   0.0      0.000  0.000       0.000      0.0   \n",
      "1     ...      0.0       0.0   0.0      0.000  0.000       0.000      0.0   \n",
      "2     ...      0.0       0.0   0.0      0.000  0.000       0.125      0.0   \n",
      "3     ...      0.0       0.0   0.0      0.000  0.000       0.000      0.0   \n",
      "4     ...      0.0       0.0   0.0      0.000  0.000       0.125      0.0   \n",
      "...   ...      ...       ...   ...        ...    ...         ...      ...   \n",
      "1147  ...      0.0       0.0   0.0      0.000  0.000       0.000      0.0   \n",
      "1148  ...      0.0       0.0   0.0     -0.125  0.000       0.000     -0.5   \n",
      "1150  ...      0.0       0.0   0.0      0.000  0.000       0.000      0.0   \n",
      "1151  ...      0.0       0.0   0.0      0.000  0.125       0.000      0.0   \n",
      "1152  ...      0.0       0.0   0.0      0.000  0.000       0.000      0.0   \n",
      "\n",
      "      favorite  simple  total_sentiment  \n",
      "0          0.0     0.0            1.250  \n",
      "1          0.0     0.0            1.375  \n",
      "2          0.0     0.0            3.625  \n",
      "3          0.0     0.0           -0.250  \n",
      "4          0.0     0.0            0.500  \n",
      "...        ...     ...              ...  \n",
      "1147       0.0     0.0           -0.625  \n",
      "1148       0.0     0.0            1.375  \n",
      "1150       0.0     0.0            0.125  \n",
      "1151       0.0     0.0            0.875  \n",
      "1152       0.0     0.0            2.125  \n",
      "\n",
      "[579 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combinaison la matrice de animation,_adj pour prépare la modélisation\n",
    "print(animation_train_df [['note']])\n",
    "animation_train_df.reset_index(drop=True, inplace=True)\n",
    "animation_adj_sentiment_matrix.reset_index(drop=True, inplace=True)\n",
    "animation_adj = pd.concat([animation_train_df [['note']], animation_adj_sentiment_matrix], axis=1)\n",
    "animation_adj['total_sentiment'] = animation_adj_sentiment_matrix.sum(axis=1)\n",
    "animation_adj = animation_adj.loc[:, (animation_adj != 0).any(axis=0)]\n",
    "animation_adj = animation_adj[animation_adj['total_sentiment'] != 0]\n",
    "animation_adj = animation_adj.dropna(subset=['total_sentiment'])\n",
    "animation_adj = animation_adj.drop_duplicates()\n",
    "animation_adj = animation_adj.apply(pd.to_numeric, errors='coerce')\n",
    "median_animation=animation_df['note'].median()\n",
    "animation_adj['note'] = binarize_by_median(animation_adj, 'note', median_animation)\n",
    "print(animation_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                   note   Log-Likelihood:                -291.68\n",
      "Model:                   OrderedModel   AIC:                             717.4\n",
      "Method:            Maximum Likelihood   BIC:                             1010.\n",
      "Date:              mer., 13 nov. 2024                                         \n",
      "Time:                        22:17:42                                         \n",
      "No. Observations:                 579                                         \n",
      "Df Residuals:                     512                                         \n",
      "Df Model:                          66                                         \n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "good             0.5163      0.296      1.742      0.082      -0.065       1.097\n",
      "bad              1.9228      0.438      4.389      0.000       1.064       2.782\n",
      "little           0.3911      0.677      0.578      0.563      -0.936       1.718\n",
      "best             0.6134      0.396      1.549      0.121      -0.163       1.389\n",
      "old              2.6116      0.918      2.846      0.004       0.813       4.410\n",
      "new             -0.0244      0.832     -0.029      0.977      -1.654       1.606\n",
      "classic          2.4270      1.003      2.420      0.016       0.461       4.392\n",
      "whole           -1.4449      2.644     -0.547      0.585      -6.627       3.737\n",
      "funny            0.1629      0.637      0.256      0.798      -1.085       1.411\n",
      "big              2.6057      2.680      0.972      0.331      -2.647       7.859\n",
      "beautiful        1.1812      0.539      2.192      0.028       0.125       2.237\n",
      "nice             0.1325      0.457      0.290      0.772      -0.764       1.029\n",
      "sure             3.7047      1.537      2.411      0.016       0.693       6.717\n",
      "last             1.0144      1.447      0.701      0.483      -1.822       3.851\n",
      "different        1.8857      0.637      2.961      0.003       0.637       3.134\n",
      "better          -0.0468      0.475     -0.099      0.921      -0.977       0.884\n",
      "main             0.3227      1.028      0.314      0.754      -1.693       2.338\n",
      "terrible         2.4697      0.953      2.592      0.010       0.602       4.337\n",
      "hard             0.4679      0.636      0.736      0.462      -0.778       1.714\n",
      "wrong            0.3245      0.678      0.478      0.632      -1.005       1.654\n",
      "musical          1.1398      1.873      0.609      0.543      -2.530       4.810\n",
      "true             8.6013      3.930      2.189      0.029       0.899      16.303\n",
      "interesting     -0.8850      1.245     -0.711      0.477      -3.325       1.555\n",
      "worth            2.0355      1.969      1.034      0.301      -1.824       5.895\n",
      "evil             0.5283      0.599      0.882      0.378      -0.646       1.703\n",
      "high             5.3400      3.442      1.551      0.121      -1.407      12.087\n",
      "happy           -0.0670      0.651     -0.103      0.918      -1.343       1.209\n",
      "enough           1.6576      3.562      0.465      0.642      -5.324       8.639\n",
      "small            1.9192      1.219      1.575      0.115      -0.470       4.308\n",
      "obvious         -1.6127      1.048     -1.539      0.124      -3.667       0.442\n",
      "decent           0.0589      0.575      0.102      0.918      -1.068       1.186\n",
      "poor             1.9360      0.646      2.996      0.003       0.669       3.203\n",
      "worst            2.8212      1.186      2.378      0.017       0.496       5.147\n",
      "enjoyable        1.1126      1.806      0.616      0.538      -2.427       4.652\n",
      "awful            2.0675      0.862      2.398      0.016       0.378       3.757\n",
      "perfect          1.1581      1.206      0.960      0.337      -1.205       3.521\n",
      "wonderful        1.2972      0.761      1.704      0.088      -0.194       2.789\n",
      "magic           -0.6908      2.605     -0.265      0.791      -5.797       4.415\n",
      "animated        -0.3722      1.352     -0.275      0.783      -3.021       2.277\n",
      "comic           -0.7620      1.268     -0.601      0.548      -3.248       1.724\n",
      "memorable       -4.8558      4.373     -1.110      0.267     -13.428       3.716\n",
      "modern           3.9101      1.917      2.040      0.041       0.154       7.667\n",
      "flat           -10.9414      5.087     -2.151      0.031     -20.912      -0.971\n",
      "disappointed     0.3013      1.084      0.278      0.781      -1.824       2.427\n",
      "stupid          -0.3439      0.640     -0.538      0.591      -1.598       0.910\n",
      "low              3.2974      2.912      1.132      0.257      -2.409       9.004\n",
      "potential        4.8641      2.239      2.172      0.030       0.475       9.253\n",
      "serious         -2.9332      4.944     -0.593      0.553     -12.623       6.757\n",
      "able             3.7960      5.177      0.733      0.463      -6.351      13.943\n",
      "dead            -1.1995      0.923     -1.299      0.194      -3.009       0.610\n",
      "amazing          1.8798      2.288      0.822      0.411      -2.604       6.363\n",
      "actual           0.0488      1.552      0.031      0.975      -2.993       3.090\n",
      "excellent        0.6339      0.633      1.001      0.317      -0.608       1.875\n",
      "negative        -1.7463      0.776     -2.250      0.024      -3.268      -0.225\n",
      "white            5.1054      2.974      1.717      0.086      -0.723      10.933\n",
      "spectacular      0.5006      2.385      0.210      0.834      -4.175       5.176\n",
      "fine            -1.4410      1.688     -0.854      0.393      -4.749       1.867\n",
      "magical         -2.5867      2.944     -0.879      0.380      -8.357       3.184\n",
      "horrible         1.9462      1.126      1.729      0.084      -0.260       4.152\n",
      "huge            -3.0040      4.140     -0.726      0.468     -11.118       5.110\n",
      "hilarious       -5.9612      4.858     -1.227      0.220     -15.483       3.560\n",
      "usual            8.2951      4.227      1.962      0.050       0.011      16.580\n",
      "impressive       3.2206      4.513      0.714      0.476      -5.626      12.067\n",
      "several          0.8327      1.298      0.642      0.521      -1.711       3.376\n",
      "favorite        -5.1422      4.788     -1.074      0.283     -14.527       4.242\n",
      "simple          -8.3366      3.000     -2.778      0.005     -14.217      -2.456\n",
      "0/1             -0.3782      0.197     -1.915      0.055      -0.765       0.009\n",
      "================================================================================\n",
      "717.3674317698449\n"
     ]
    }
   ],
   "source": [
    "#Modèle Logit Ordonnance \n",
    "y = animation_adj['note']\n",
    "X = animation_adj.drop(columns=['note','total_sentiment'])\n",
    "model_inital = OrderedModel(y, X, distr='logit')\n",
    "result = model_inital.fit(method='bfgs', disp=False)\n",
    "aic_initiale=result.aic\n",
    "print(result.summary())\n",
    "print(aic_initiale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
